Int. J. Production Economics 154 (2014) 72–80

Contents lists available at ScienceDirect

Int. J. Production Economics
journal homepage: www.elsevier.com/locate/ijpe

Data quality for data science, predictive analytics, and big data
in supply chain management: An introduction to the problem
and suggestions for research and applications
Benjamin T. Hazen a,n, Christopher A. Boone b, Jeremy D. Ezell c, L. Allison Jones-Farmer c
a
Department of Marketing and Supply Chain Management, College of Business Administration, 310 Stokely Management Center, University of Tennessee,
Knoxville, TN 37996-0530, USA
b
Department of Marketing and Logistics, College of Business Administration, Georgia Southern University, 1332 Southern Drive, Statesboro, GA 30458, USA
c
Department of Supply Chain and Information Systems Management, Harbert College of Business, 401 Lowder Building, 415 West Magnolia Avenue,
Auburn University, Auburn, AL 36849, USA

article info

abstract

Article history:
Received 17 October 2013
Accepted 12 April 2014
Available online 26 April 2014

Today's supply chain professionals are inundated with data, motivating new ways of thinking about how
data are produced, organized, and analyzed. This has provided an impetus for organizations to adopt and
perfect data analytic functions (e.g. data science, predictive analytics, and big data) in order to enhance
supply chain processes and, ultimately, performance. However, management decisions informed by the
use of these data analytic methods are only as good as the data on which they are based. In this paper,
we introduce the data quality problem in the context of supply chain management (SCM) and propose
methods for monitoring and controlling data quality. In addition to advocating for the importance of
addressing data quality in supply chain research and practice, we also highlight interdisciplinary
research topics based on complementary theory.
& 2014 Elsevier B.V. All rights reserved.

Keywords:
Data quality
Statistical process control
Knowledge-based view
Organizational information processing view
Systems theory

1. Introduction
Many have argued that the market focus of competition has
evolved from that of competition between individual ﬁrms to
competition between entire supply chains (Craighead et al., 2009;
Ketchen and Hult, 2007; Slone, 2004; Whipple and Frankel, 2000).
The resulting focus on supply chain management (SCM) has forced
managers to rethink their competitive strategies (Zacharia et al.,
2011), with many now seeking to “win with data” (Hopkins et al.,
2010). Supply chain managers are increasingly reliant upon data to
gain visibility into expenditures, identify trends in costs and
performance, and support process control, inventory monitoring,
production optimization, and process improvement efforts. In fact,
many businesses are awash in data, with many seeking to
capitalize on data analysis as a means for gaining a competitive
advantage (Davenport, 2006). Data science, predictive analytics,
and “big data” are each thought to be part of an emerging
competitive area that will transform the way in which supply
chains are managed and designed (Waller and Fawcett, 2013).



The emerging ﬁeld of data science combines mathematical,
statistical, computer science, and behavioral science expertise to
tease insights from enterprise data, while predictive analytics
describes the set of data science tools leveraged for future outcome prediction attempts (Barton and Court, 2012; Davenport and
Patil, 2012). Big data is a more tenuous term, its deﬁnition and
usage changing to include more than just the size or volume of the
organization's data but the variety and velocity as well (Megahed
and Jones-Farmer, 2013). As coined by Waller and Fawcett (2013),
we collectively refer to these three related topics as data science,
predictive analytics, and big data (DPB). Considering both the
proliferation of DPB activities for supply chain management and
the fact that the data upon which these DPB functions rely are
often plagued with errors (Dey and Kumar, 2010), there is an
important need to examine the data quality problem as it pertains
to the ﬁeld of SCM.
In the epic poem Rime of the Ancient Mariner, Samuel Taylor
Coleridge states, “Water, water, everywhere, nor any a drop to
drink.” Data embodies the same degree of uselessness for consumption if it is of poor quality. Indeed, the degree to which data
can be used is largely determined by their quality (O’Reilly, 1982).
Poor quality data can have a direct impact on business decisions
(Dyson and Foster, 1982; Warth et al., 2011) and have been shown
to promote a number of tangible and intangible losses for
businesses (Batini et al., 2009). The costs of poor data quality have

B.T. Hazen et al. / Int. J. Production Economics 154 (2014) 72–80

been estimated to be as high as 8% to 12% of revenues for a typical
organization and may generate up to 40% to 60% of a service
organization's expenses (Redman, 1998); this translates into losses
that are estimated to exceed billions of dollars per year (Batini and
Scannapieco, 2006; Dey and Kumar, 2010). Poor data quality can
be equally damaging to less tangible areas including job satisfaction, decision quality, and propagation of mistrust between and
within organizations (Redman, 1996). Supply chain managers are
seeing the problems and impacts attributed to poor data quality
growing in importance. Although high quality data has always
been a must-have for these managers, quality issues are increasing
as ﬁrms’ desires and capability for the analysis of ever larger
amounts of acquired data similarly increase (Parssian et al., 2004).
In fact, a recent survey of over 3000 business executives found that
one in ﬁve executives consider data quality as a primary obstacle
to adopting more robust data analytic-based strategies (Lavalle et
al., 2011).
The goal of this paper is to introduce and stress the need for the
monitoring and control of data quality in supply chain management processes and provide a starting point for future research
and applications. The remainder of this paper is structured as
follows. The paper begins with a short overview of the data
production process. We then describe data quality and deﬁne its
constituent dimensions. Then, we discuss methods for monitoring,
controlling, and improving data quality and use a practical
example of how one organization employed such a method to
enhance data quality in its supply chain. We next examine how the
data quality problem can be viewed through the lenses of systems
theory, the knowledge-based view, and organizational information
processing view to provide a series of theory-based topics to guide
future research. Finally we discuss both managerial implications
and additional research considerations.

73

is no control stage recommended in the TDQM cycle. With no
means for controlling the quality of data, there is no framework for
inducing continuous improvement in the data production process.
Two important contributions of this paper are to introduce the
need for continuous improvement in the SCM data production
process, and to suggest a familiar framework for establishing a
quality control mechanism regarding data quality.
The analogy of the data production process to a manufacturing
process is, perhaps, one of the most widely accepted views in the
literature. Although there are many similarities between the data
production process and the manufacturing process, we will discuss
what we feel are two of the most important differences. In a
manufacturing process, raw materials are input into a process, the
materials are transformed, and the resulting output is a manufactured product. The raw materials are generally depleted as the
goods are produced. In the data production process, the data
represent the input into the data production process, and a
transformed data product is the output of the production process.
The data are generally not depleted through production. A bad
batch of data in the data production process will remain until it is
actively cleaned up or removed. Perhaps the most pertinent, yet
challenging difference between a manufacturing and data production process relates to the difﬁculty with regard to measuring the
quality of intangible data. A common phrase of quality control
practitioners is “you cannot improve that which you cannot
measure.” Thus, some attempt must be made to operationally
deﬁne and measure data quality. As with measuring the quality of
a physical product, data quality is a multidimensional problem
(Garvin, 1984, 1987). In the next section we review the mainstream literature on the dimensions of data quality to gain insight
into the most important quality characteristics.

3. Dimensions of data quality
2. Data production
Several scholars have drawn an analogy between product
manufacturing and “data manufacturing” (Arnold, 1992; Ballou
et al., 1998; Emery, 1969; Huh et al., 1990; March and Hevner,
2007; Ronen and Spiegler, 1991; Wang and Kon, 1993; Wang et al.,
1995). For instance, Wang et al. (1995) proposed a simple framework, depicted in Fig. 1, to describe the similarities between the
two manufacturing processes.
Wang (1998) extended this comparison between data and
manufacturing processes by suggesting data quality should be
addressed via a Total Data Quality Management (TDQM) cycle,
which calls for continuously deﬁning, measuring, analyzing, and
improving data quality. This approach is similar to Deming’s
(2000) Total Quality Management cycle (Plan, Do, Check, and
Act) and analogous to the Deﬁne, Measure, Analyze, Improve,
Control (DMAIC) cycle, as ascribed by Six Sigma, for the data
manufacturing process. However, Jones-Farmer et al. (2013)
pointed out that, unlike the DMAIC cycle from Six Sigma, there

Research suggests that data quality is comprised of several
dimensions (Ballou and Pazer, 1985; Ballou et al., 1998; Pipino et
al., 2002; Redman, 1996; Wand and Wang, 1996; Wang and Strong,
1996). Both Wang and Strong (1996) and Lee et al. (2002) organize
data quality dimensions into two categories: intrinsic, referring to
attributes that are objective and native to the data and contextual,
referring to attributes that are dependent on the context in which
the data are observed or used. Contextual dimensions include
relevancy, value-added, quantity (Wang and Strong, 1996), believability, accessibility, and reputation of the data (Lee et al., 2004,
2002). Measures of these dimensions have relied heavily on selfreport surveys and user questionnaires, as they rely on subjective
and situational judgments of decision makers for quantiﬁcation
(Batini et al., 2009). Contextual dimensions of data quality lend
themselves more towards information as opposed to data, because
these dimensions are formed by placing data within a situation or
problem speciﬁc context (Batini et al., 2009; Davenport and
Prusak, 2000; Haug et al., 2009; Watts et al., 2009). Because we

Fig. 1. An analogy between product and data manufacturing processes (Wang et al., 1995).

74

B.T. Hazen et al. / Int. J. Production Economics 154 (2014) 72–80

Table 1
Dimensions of data quality.
Data quality
dimension

Description

Accuracy

Are the data free of errors?

Timeliness
Consistency
Completeness

Supply chain example

Customer shipping address in a customer relationship management system matches the address on the most
recent customer order
Are the data up-to-date?
Inventory management system reﬂects real-time inventory levels at each retail location
Are the data presented in the same All requested delivery dates are entered in a DD/MM/YY format
format?
Are necessary data missing?
Customer shipping address includes all data points necessary to complete a shipment (i.e. name, street
address, city, state, and zip code)

consider the quality of data, not information, as it moves through a
production-like process, we limit our discussion of quality to
consideration of the intrinsic measures of data quality.
The literature consistently describes intrinsic data quality along
four dimensions: accuracy; timeliness; consistency; and completeness (Ballou and Pazer, 1985; Batini et al., 2009; Blake and
Mangiameli, 2011; Haug and Arlbjørn, 2011; Haug et al., 2009;
Kahn et al., 2002; Lee et al., 2002; Parssian, 2006; Scannapieco and
Catarci, 2002; Wang and Strong, 1996; Zeithaml et al., 1990).
Below, we explore and use the aforementioned literature to deﬁne
and describe these four dimensions.
Accuracy refers to the degree to which data are equivalent to
their corresponding “real” values (Ballou and Pazer, 1985). This
dimension can be assessed via comparing values with external
values that are known to be (or considered to be) correct (Redman,
1996). A simple example would be a data record in a customer
relationship management system, where the street address for a
customer in the system matches the street address where the
customer currently resides. In this case, accuracy of the street
address value in the system could be assessed via validating the
shipping address on the most recent customer order. No problem
context or value-judgment of the data is needed: it is either
accurate or not. Its accuracy is entirely self-dependent.
Timeliness refers to the degree to which data are up-to-date.
Research suggests that timeliness can be further decomposed into
two dimensions: (1) currency, or length of time since the record's
last update, and (2) volatility, which describes the frequency of
updates (Blake and Mangiameli, 2011; Pipino et al., 2002; Wand
and Wang, 1996). Data that are correct when assessed, but
updated very infrequently, may still hamper efforts at effective
managerial decision making (e.g., errors that occur in the data may
be missed more often than not with infrequent record updating,
preventing operational issues in the business from being detected
early). A convenient example measure for calculating timeliness
using values for currency and volatility can be found in Ballou et al.
(1998), p. 468, where currency is calculated using the time of data
delivery, the time it was entered into the system, and the age of
the data at delivery (which can differ from input time). Together,
currency and volatility measures are used to calculate timeliness.
Consistency refers to the degree to which related data records
match in terms of format and structure. Ballou and Pazer (1985)
deﬁne consistency as when the “representation of the data value is
the same in all cases” (p. 153). Batini et al. (2009) develop the notion
of both intra-relation and inter-relation constraints on the consistency of data. Intra-relation consistency assesses the adherence of
the data to a range of possible values (Coronel et al., 2011), whereas
inter-relation assesses how well data are presented using the same
structure. An example of this would be that a person, currently
alive, would have for “year of birth” a possible value range of 1900–
2013 (intra-relation constraint), while that person's record in two
different datasets would, in both cases, have a ﬁeld for birth year,
and both ﬁelds would intentionally represent the person's year of
birth in the same format (inter-relation constraint).

Completeness refers to the degree to which data are full and
complete in content, with no missing data. This dimension can
describe a data record that captures the minimally required
amount of information needed (Wand and Wang, 1996), or data
that have had all values captured (Gomes et al., 2007). Every ﬁeld
in the data record is needed in order to paint the complete picture
of what the record is attempting to represent in the “real world.”
For example, if a particular customer's record includes a name and
street address, but no state, city, and zip code, then that record is
considered incomplete. The minimum amount of data needed for a
correct address record is not present. A simple ratio of complete
versus incomplete records can then form a potential measure of
completeness.
A summary of the dimensions of data quality is presented in
Table 1. Once data quality measures are understood, these quality
measures can be monitored for improvement or adherence to
standards. For example, data can be tagged as either accurate or
not. Once tagged, there should be a method in place to monitor the
long-term accuracy of the data. Combined with the measuring and
monitoring the other three data quality dimensions, this helps to
ensure that the records in the dataset are as accurate, timely,
complete, and consistent as is practical.
Understanding the four intrinsic dimensions of data quality
allows us to operationally deﬁne measures for these dimensions
and apply tools to actively monitor for data quality problems.
For instance, total quality management approaches (Porter and
Rayner, 1992; Redman, 1992), process capability analyses
(Veldman and Gaalman, 2013), statistical process control (SPC),
and additional quality tools and theories might help inform data
quality management techniques, and investigation into using
these techniques in the context of the data quality problem is
needed. To this end, tools from SPC have been suggested as a
natural ﬁt for monitoring and improving data quality over time
(Jones-Farmer et al., 2013). In particular, control charts can be used
to improve data quality, not batch-by-batch, but in the overall data
production process. Although there are several quality methods
that should be examined in future data quality research, we
suggest that SPC control chart methods might be most useful as
an illustrative example of controlling and monitoring data quality
in a supply chain DPB setting. Thus, in the next section we describe
details regarding how SPC control chart methods can be used to
monitor and control data quality in a supply chain, and provide a
brief example case study.

4. Controlling data quality with SPC
Both academic and practitioner literature has stated the need
for improved data quality for effective management and decision
making (Redman, 1998). To this end, much of the research has
examined ways in which to assess the quality of data products
after they are created (Dey and Kumar, 2010; Parssian et al., 2004).
Although useful, this practice is akin to quality checking ﬁnished

B.T. Hazen et al. / Int. J. Production Economics 154 (2014) 72–80

75

Fig. 2. An example Shewhart-type control chart showing an in-control process.

products at the end of a production line. Like with manufacturing,
monitoring and controlling for quality throughout the duration of
the data production process might be a more useful endeavor as
deﬁciencies can be addressed in real time and corrected before
they create cascading defects.
Redman (1992, 1996, 2001) emphasizes the importance of
maintaining a process focus when considering data quality. He
advocates the use of simple tools such as the histogram, ﬁshbone
diagram, and Pareto chart for cleaning up a data production
process. For more information on the application of simple quality
tools in general, the interested reader is referred to Montgomery
(2013) or speciﬁc to data quality, Redman (1992). Once the initial
quality efforts have improved the current state of data quality,
bringing it into an in-control state, ongoing process monitoring
should be used to maintain the required level of data quality.
Unfortunately, as discussed above, there is no such control stage
proposed in Wang’s (1998) TDQM cycle.
Advanced control chart methodologies provide long established and commonly employed methods for monitoring and
controlling production quality (Lieberman, 1965; Mitra, 2008;
Woodall, 2000). Indeed, SPC, topics are often discussed in the
SCM literature (e.g. Choi and Rungtusanatham, 1999; Fraisat and
Sawalha, 2013; Laframboise and Reyes, 2005; Rahman, 2006; Sila
et al., 2006). While SPC methods are common in SCM, these
methods are not commonly applied in the production of data used
in support of managing a supply chain. Therefore, we propose SPC
methods as one means by which to monitor, control, and ultimately improve the quality of data used for SCM.
We suspect the lack of commonplace application of advanced
SPC methods to monitoring data quality is due, in part, to the lack
of awareness of the applicability of the methods on the part of
practitioners, but also because SPC methods were developed based
on assumptions relevant to the actual processes themselves, and
not necessarily the data by which managers use to control these
processes. We focus on the data production process, which
includes data collection, storage, retrieval, and processing. We
consider the output of this process, a data set, as a product, much
like a product of a manufacturing process (Wang et al., 1995).
Similar to those who examine Six Sigma in a manufacturing
environment (e.g. Zu et al., 2010), we are motivated to examine

how the data production process can be effectively monitored,
controlled, and improved using control charts for the purpose of
enhancing the quality of the data supply chain professionals use to
manage their processes.
Shewhart (1931) ﬁrst introduced control charts as a method for
monitoring the output quality of manufacturing processes. The
methods were popularized following World War II as Deming
(2000) used control charts to aid the Japanese in rebuilding their
manufacturing base. Fig. 2 gives an example of a simple Shewharttype control chart. The dotted lines labeled UCL and LCL represent
upper and lower control limits, respectively, and are based on the
statistical nature of the process under consideration. Each data
series plotted over time represents a measure of a process
characteristic. Values that fall between the UCL and LCL are
considered subject to only usual or common-cause process variation. When all points in a process fall between the control limits,
the process is considered to be in-control. Points that plot either
above the UCL or below the LCL are considered signals to a
potential out-of-control scenario, or affected by some force that
is not expected within the usual operating conﬁnes of the process.
When a control chart signals a potential out-of-control event, the
process operators investigate for possible root causes of the signal.
For more information about control charts, we refer the reader to
Montgomery (2013).
Researchers in SPC have advanced the ﬁeld of control charts far
beyond the traditional Shewhart charts like the one illustrated
above. More advanced control charts include the Cumulative Sum
(CUSUM) (Page, 1961), the Exponentially Weighted Moving Average (EWMA) (Roberts, 1959), multivariate Shewhart-type control
charts such as Hotelling’s (1947) T2 chart , multivariate versions of
the CUSUM (Crosier, 1998), and EWMA (Lowry et al., 1992), and
many others (Ho and Quinino, 2013; Ou et al., 2012; Wu et al.,
2009). In addition, many control charts have been developed to
monitor categorical and discrete process characteristics (see, e.g.,
Woodall (1997) and Topalidou and Psarakis (2009)). Like most
statistical methods, the different control charts are designed to
work in different scenarios with different types of data. JonesFarmer et al. (2013) give an overview of the advanced control
charting tools that are applicable to monitoring the intrinsic
measures of data quality.

76

B.T. Hazen et al. / Int. J. Production Economics 154 (2014) 72–80

We found evidence of some use of control charts for monitoring data quality (Pierchala et al., 2009; Redman, 2001; Sparks and
OkuGami, 2010); however, the applications have not been widely
adopted by practitioners. Indeed, there are many opportunities for
advancement in the practical application of control charts to data
quality in the SCM context. With the proliferation of DPB and the
increasingly important role of the supply chain in the success of
today's businesses, we suggest the use of control charts for
monitoring supply chain data quality. Next, we demonstrate how
such an approach might be employed to enhance data quality in a
supply chain setting.
4.1. Use of SPC to monitor and control supply chain data: An example
case study
As part of a ﬁeld-based research effort, we examined the data
management program of an organization that remanufactures jet
engines and related components for military aircraft and introduced the use of control chart methods to enhance data quality.
This particular data management system is used for real-time
monitoring of the closed-loop jet engine inventory for one
particular cargo aircraft. Engine location and repair status are
among some of the important information tracked in this

database. Data products derived from this system are used by line,
middle-, and senior-level managers at several locations for a wide
variety of decision-making purposes (e.g. to determine if a
particular aircraft is currently capable to deploy overseas in the
sense that none of its engines require extensive maintenance or
inspections that deployed locations are incapable of conducting).
For the purpose of this illustration, we limit our examination to
data records regarding jet engine compressors (which is one subcomponent of a jet engine). As described below, records for eight
different compressors were captured in real-time. To maintain
brevity in our example, we focus here on one of the four intrinsic
data quality dimensions: completeness. Completeness was measured at the record level and deﬁned as
(
0 if record is complete
C i:k ¼
1 if record is incomplete
for i ¼ 1; :::; 8, compressors and k ¼ 1; :::; N R , part records. Thus, we
have eight binary variables describing completeness.
The ﬁrst 400 observations taken can be used as the reference
sample. Table 2 shows the phi coefﬁcients (Cohen et al., 2003)
estimating the correlation among the eight completeness variables. The values given along the diagonal are the estimated
proportion of incomplete records.


Note: All table values are estimated from 400 reference sample observations. The values along the diagonal estimate the percent of incomplete records for each of eight
compressors. The lower diagonal observations represent the correlation between each row and column variable. The notations, n, nn, and nnn indicate signiﬁcance at the .10,
.05, and .01 level of signiﬁcance, respectfully.

Fig. 3. Bernoulli CUSUM chart of completeness of component 6 for aircraft maintenance database.

B.T. Hazen et al. / Int. J. Production Economics 154 (2014) 72–80

Fig. 3 shows a control chart of the completeness scores for the
next 204 observations of compressor 6. Here, we chose to use a
Bernoulli CUSUM control chart to monitor the incomplete records.
Because each record was determined either incomplete (1) or
complete (0), the data may be well-modeled by a Bernouli
distribution. The use of the CUSUM control chart for individual
(as opposed to subgrouped) Bernoulli random variables was
originally proposed by Reynolds and Stoumbos (1999) as a fastdetection alternative to the Shewhart p-chart for continuous
monitoring of dichotomous variables. The use of the Bernoulli
CUSUM for monitoring dichotomous quality characteristics can
lead to a faster detection of process changes because it eliminates
the need to accumulate a large subgroup of observations prior to
plotting a control chart statistic. As can be seen from the chart,
out-of-control signals were given on observations 445–448 following a series of incomplete records. Using this chart, managers
were able to detect a data quality problem. Following observation
448, corrective action was taken that included retraining the data
entry workforce. At that point, the CUSUM chart was reset, and the
process continued in an in-control state.
The example described above is brief in nature for the purpose
of description in this paper. It should be noted that this method
can be implemented on a large scale to examine data products
generated from a data warehouse on all four dimensions of data
quality. Although we do not go into speciﬁc detail here, the
interested reader is referred to Jones-Farmer et al. (2013) for a
deeper discussion and speciﬁc details on applying control chart
methodology to monitor and improve the data production process.
Now that we have outlined the data production process,
described the dimensions of data quality, and demonstrated a
practical means for controlling data quality using an SPC framework, we turn to a discussion of future research needs in the area
of enhancing data quality for DPB in the supply chain. In the next
section, we highlight applicable theory that might help inform
future research in this area and propose topics in need of further
exploration.

5. Theory-based research opportunities
We propose that considering data quality should be common
practice in future supply chain DPB research. At a minimum, data
quality should be acknowledged, measured, monitored, and controlled. Ideally, the goal of any measurement or monitoring
activity would be to improve the quality of the process and
product. We believe that simply acknowledging, measuring, and
monitoring the quality of a ﬁrm's SCM data will lead to inevitable
improvement in the quality of the data. Further, establishing an
ongoing monitoring scheme will allow for future data acquisitions
to be controlled, with the goal of improving the quality of the data
and ultimately the decisions that are based on the data.
Although our consideration thus far has been on the technical
nature of data quality, there are also speciﬁc and theoretically
based research questions pertinent to data quality and SCM that
can and should also be addressed in future research. While there
are surely several theories that can be used as a basis to study
emerging problems regarding data quality in the context of DPB
and SCM, as a starting point we highlight three. More speciﬁcally,
we frame some example research questions in the context of the
knowledge-based view (KBV), systems theory, and the organizational information processing view (OIPV).
5.1. Knowledge-based view
The resource-based view (RBV) suggests that organizations
create competitive advantage via employment of the resources

77

at their disposal (Barney, 1991). The theory is often used to frame
research in the SCM domain to describe competition between
ﬁrms and supply chains (Defee et al., 2010; Fawcett and Waller,
2011). Most importantly, to translate short-term competitive in
to sustained competitive advantages, resources must be valuable
(the resource enables the ﬁrm to create value for customers), rare
(the resource is not widely available), inimitable (the resource
cannot be easily replicated or procured by other ﬁrms), and nonsubstitutable (other ﬁrms cannot employ an alternative resource
that offers similar utility or drives the resource into obsolescence)
(Barney, 1991).
As an extension of RBV, the KBV considers knowledge as one
such resource that can be valuable, rare, inimitable, and nonsubstitutable (Grant, 1996). The value of DPB as a knowledge
resource and its impact on the ﬁrm's competitive advantage in the
market are both dependent on data quality. This highlights the
importance of the consideration of a ﬁrm's data quality levels and
processes. For instance, a DPB effort might not necessarily create
value for a ﬁrm if a determined baseline level of data quality is not
attained. Conversely, a high level of data quality might enable DPB
efforts that are rare, inimitable, and perhaps non-substitutable
among competitors. Considering the KBV, we suggest the following research questions:

 Is there a relationship between perceived or known levels of
data quality and DPB usage in supply chain applications?

 Does data quality play an intervening role in the relationship
between DPB activities and measures of supply chain
performance?
 If DPB activities can be considered a knowledge resource, then
when, how, and how often should ﬁrms conduct an analysis of
the quality of their data in order to maintain this knowledge
resource?
 Similar to the above, precisely how can data quality analyses
affect the value of both (a) a ﬁrm's data and (b) a ﬁrm's DPB
activities as knowledge resources capable of enhancing supply
chain performance?
 How does the quality of the data affect perceived value placed
on DPB efforts when making strategic decisions regarding
competitive actions?

5.2. Systems theory
Systems theory is another commonly used theory in the SCM
literature (Chicksand et al., 2012; Ketchen and Hult, 2007) that
might provide a useful lens through which to view the data quality
problem. System theory suggests that organizations are open and
porous systems that interact with their surrounding environment,
and thus are continually evolving (Von Bertalanffy, 1951). From
this perspective, it is easy to envision a supply chain as a system of
connected nodes (Towill et al., 1992) that are interacting with and
relying on inputs from the external environment and from each
other (Blackhurst et al., 2011). Similarly, information systems that
support DPB can be viewed as SCM sub-systems operating within
a real-world feedback control system (Orr, 1998). This is because
such systems are by nature intra-organizational and rely upon
interactions with and inputs from a variety of both inter- and
intra-ﬁrm actors.
Orr (1998) suggests that there must be a mechanism to
synchronize system data with changes in the environment. As
the system absorbs data at all node points, the quality of that data
could change as rapidly as the volume being gathered. As such, the
measurement and control of data quality might be especially
important from a systems theory perspective when investigating

78

B.T. Hazen et al. / Int. J. Production Economics 154 (2014) 72–80

the impact of DPB on SCM performance. Considering systems
theory, we suggest the following research questions:

 How can organizations integrate data quality and control
initiatives into their existing or emerging supply chain DPB
programs?
 What are the costs of poor data quality to speciﬁc supply chain
DPB initiatives? What are the costs of maintaining “acceptable”
data quality, and do these costs result in an adequate return on
investment? How is “acceptable” data quality deﬁned for
supply chain operations?
 How does one ﬁrm's data quality affect the DPB efforts of
partner ﬁrms within the supply chain?
 Considering the boundary-spanning role of logistics and supply
chain operations, how does the level of data quality affect ﬁrm
processes outside of logistics and supply chain operations?

5.3. Organizational information processing view
The OIPV suggests that organizations are imperfect decision
making systems due to incomplete information, which is a function of uncertainty and equivocality (March and Simon, 1958).
While uncertainty refers to incomplete knowledge, equivocality
results from conﬂicting interpretations about a decision-making
situation (Daft and Lengel, 1986; Galbraith, 1974). Levels of data
quality might play a role in regard to the completeness of knowledge and interpretations of information.
The OIPV considers three primary components: information
processing needs, information processing capabilities, and the ﬁt
between needs and capabilities (Tushman and Nadler, 1978).
Information processing needs indicate the information required
by the organization to enable effective decision-making, whereas
information processing capabilities indicate the organization's
actual capacity to structure and utilize information to support
decision-making (Tushman and Nadler, 1978). Fit is the degree to
which a ﬁrm's information processing capabilities satisfy its
information processing needs (Tushman and Nadler, 1978). It is
via ﬁt that ﬁrms can reduce uncertainty and equivocality, enhance
decision-making capabilities and, subsequently, enhance performance (Trautmann et al., 2009; Wu et al., 2013). As such, OIPV can
be a useful lens through which to examine how efforts to enhance
data quality might lead to increased levels of performance realized
via DPB. Considering the OIPV, we suggest the following research
questions:

 Does the degree of data quality play an intervening role in the
relationship between supply chain information processing
needs and capabilities?
 Does the degree of data quality impact the range of information
processing needs? Does enhanced data quality cause a re-scope
of processing needs as processing capabilities are enhanced?
 Does enhancing data quality help to reduce uncertainties
surrounding DPB activities used for SCM?
6. Implications and concluding remarks
As alluded to above, the study of DPB in general and the data
quality problem in particular require interdisciplinary collaboration in
order to advance. For instance, information systems experts are
needed to provide insight into how data are collected, stored,
processed, and retrieved. SCM domain experts are needed to ensure
that the right problems are the analysis being performed and results
derived thereof are relevant (Waller and Fawcett, 2013). Additionally,
emerging data quality research suggests the need for statistical and

analytical experts who are knowledgeable in methods required to
measure, monitor, and control data quality (Jones-Farmer et al., 2013).
Working together, scholars from these and other disciplines can
employ the right technologies and techniques to solve the right
problems.
Even though the ﬁeld-based effort described herein provides
support for the application of data quality control methods to data
products in a supply chain environment, more study is needed. For
example, an audit released by the United States Air Force (USAF) (Air
Force Audit Agency, 2013) reviewed the data entered by both organic
and contract maintenance personnel into one of two ﬁeld level
maintenance information systems (determined by the aircraft being
repaired). Data from these two ﬁeld level systems are automatically
transferred to an enterprise level system that serves as the USAF’s
single source of aircraft maintenance data. The enterprise level system
is used by the USAF in determining the need for weapon system
inspections and provides information on weapon system status,
utilization, and conﬁguration. Unfortunately, the audit revealed errors
in the reporting of over 30% of the observed maintenance actions.
Auditors found that many of the maintenance actions entered into the
system were incomplete or inaccurate, which resulted in the improper
grounding of aircraft and an increase in maintenance manpower costs.
Chief among the causes of the errors identiﬁed in the audit was a
failure to monitor the data transfer between the systems and a failure
to establish effective monitoring and control processes for notifying
and correcting data errors. Similar to the ﬁeld-based example presented earlier, this scenario represents an opportunity to examine
methods to improve the quality of data products.
However, data quality issues and the opportunities for the
application of data quality control methods are not isolated to
military or maintenance intensive operations. For example, the
proliferation of radio frequency identiﬁcation provides ﬁrms with a
large amount of data to enhance visibility throughout the supply
chain; unfortunately, the data generated via such technology is often
rife with errors (Delen et al., 2007). The lack of common data format
standards and the transfer of data between dissimilar systems also
plague supply chain ﬁrms that rely upon DPB to drive growth and
innovation. Errors in customers’ addresses for example can result in
advertisements and shipments being sent to the wrong customers,
which could lead to the loss of potential sales and poor customer
service. Because customer service is shown to be a key antecedent to
ﬁrm performance in the supply chain (Leuschner et al., 2013), such
poor service can indeed be detrimental. Thus, researchers should
seek to study the application of data quality control methods in a
variety of supply chain environments.
The increasing importance of data to supply chain managers
should lead to an ampliﬁed awareness and sensitivity to their need
for high quality data products. The results of decisions based on
poor quality data could be costly. Thus, supply chain managers
should begin to view the quality of the data products they depend
upon for decisions in much the same way they view the quality of
the products their supply chain delivers. Managers who appreciate
the value of data products that are accurate, consistent, complete,
and timely should consider the potential of using control methods
to improve the quality of data products much like these methods
improved the quality of manufactured products.
In this paper, we have presented a review of the literature on data
quality from the perspective of DPB in the supply chain. This review
includes literature that frames data production as a process and
deﬁnes measures of data quality. We introduce application of SPC
methods as a means to control data quality in the supply chain, and
suggest theory-based topics for future research. We hope that our
introduction to the data quality problem in the context of DPB in the
supply chain will encourage interdisciplinary collaboration to further
develop tangible methods for controlling data, and examining the
effects thereof.