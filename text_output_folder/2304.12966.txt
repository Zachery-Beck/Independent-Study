arXiv:2304.12966v1  [cs.LG]  25 Apr 2023Towards Theoretical Understanding of Inverse Reinforceme nt Learning
Alberto Maria Metelli1Filippo Lazzati1Marcello Restelli1
Abstract
Inverse reinforcement learning (IRL) denotes a
powerful family of algorithms for recovering a
reward function justifying the behavior demon-
strated by an expert agent. A well-known limita-
tion of IRL is the ambiguity in the choice of the
reward function, due to the existence of multiple
rewards that explain the observed behavior. This
limitation has been recently circumvented by for-
mulating IRL as the problem of estimating the
feasible reward set , i.e., the region of the rewards
compatible with the expert’s behavior. In this pa-
per, we make a step towards closing the theory
gap of IRL in the case of ﬁnite-horizon problems
with a generative model. We start by formally in-
troducing the problem of estimating the feasible
reward set, the corresponding PAC requirement,
and discussing the properties of particular classes
of rewards. Then, we provide the ﬁrst minimax
lower bound on the sample complexity for the
problem of estimating the feasible reward set of
orderΩ´
H3SA
ǫ2`
log`1
δ˘
`S˘¯
, beingSandA
the number of states and actions respectively, H
the horizon, ǫthe desired accuracy, and δthe con-
ﬁdence. We analyze the sample complexity of a
uniform sampling strategy ( US-IRL ), proving a
matching upper bound up to logarithmic factors.
Finally, we outline several open questions in IRL
and propose future research directions.
1. Introduction
Inverse reinforcement learning (IRL) aims at efﬁciently
learning a desired behavior by observing an expert agent
and inferring their intent encoded in a reward func-
tion (refer to Osa et al. (2018 );Arora & Doshi (2021 );
Adams et al. (2022 ) for recent surveys on IRL). This ab-
*Equal contribution1Politecnico di Milano, 32, Piazza
Leonardo da Vinci, Milan, Italy. Correspondence to: Albert o
Maria Metelli <albertomaria.metelli@polimi.it >.
Proceedings of the 39thInternational Conference on Machine
Learning , Honolulu, Hawaii, USA. PMLR 202, 2023. Copyright
2023 by the author(s).stract setting, that diverges from standard reinforcement
learning (RL, Sutton & Barto ,1998 ), as the reward func-
tion has to be learned, arises in a large variety of real-worl d
tasks. In particular, in a human-in-the-loop (Wu et al. ,
2022 ) scenario, when the expert is represented by a hu-
man solving a task, an explicit speciﬁcation of the reward
function representing the human’s goal is often unavail-
able. Experience suggests that humans are uncomfortable
when asked to describe their intent and, thus, the under-
lying reward; while they are much more comfortable pro-
viding demonstrations of what is believed to be the right
behavior. Indeed, human behavior is usually the product
of many, possibly conﬂicting, objectives.1Succeeding in
retrieving a representation of the expert’s reward has no-
table implications. First, we obtain explicit information for
understanding the motivations behind the expert’s choices
(interpretability ). Second, the reward can be employed in
RL to train artiﬁcial agents, under shifts in the features of
the underlying system ( transferability ).
Since the beginning, the community recognized that
the IRL problem is, per se, ill-posed , as multiple re-
ward functions are compatible with the expert’s behav-
ior (Ng & Russell ,2000 ). This ambiguity was heteroge-
neously addressed by the algorithmic proposals that have
followed over the years, which realized in several se-
lection criteria, including maximum margin ( Ratliff et al. ,
2006 ), maximum entropy ( Zeng et al. ,2022 ), minimum
Hessian eigenvalue ( Metelli et al. ,2017 ). Some of these ap-
proaches come with theoretical guarantees on the sample
complexity, although according to different performance
indexes (e.g., Abbeel & Ng ,2004 ;Syed & Schapire ,2007 ;
Pirotta & Restelli ,2016 ).
A promising line of research that aspires to over-
come the ambiguity issue has been recently investigated
in (Metelli et al. ,2021 ;Lindner et al. ,2022 ). These works
focus on estimating allthe reward functions compatible
with the expert’s demonstrated behavior, namely the fea-
sible rewards . Remarkably, this viewpoint that focuses on
thefeasible reward set , rather than on onereward obtained
with a speciﬁc selection criterion, as previous works did,
circumvents the ambiguity problem, postponing the reward
1In RL, the Sutton’s hypothesis ( Sutton & Barto ,1998 ) con-
jectures that a scalar reward is an adequate notion of goal.Towards Theoretical Understanding of Inverse Reinforceme nt Learning
selection and pointing to the expert’s intent. Although the se
works provide sample complexity guarantees in different
settings, a rigorous understanding of the inherent complex -
ity of the IRL problem is currently lacking.
Contributions In this paper, we aim at taking a step to-
ward the theoretical understanding of the IRL problem. As
in (Metelli et al. ,2021 ;Lindner et al. ,2022 ), we consider
the problem of estimating the feasible reward set. We focus
on a generative model setting, where the agent can query
the environment and the expert in any state, and consider
ﬁnite-horizon decision problems. The contributions of the
paper can be summarized as follows.
• We propose a novel framework to evaluate the accuracy
in recovering the feasible reward set, based on the Haus-
dorff metric (Rockafellar & Wets ,1998 ). This tool gen-
eralizes existing performance indexes. Furthermore, we
show that the feasible reward set enjoys a desirable Lip-
schitz continuity property w.r.t. the IRL problem (Sec-
tion3).
• We devise a PAC (Probability Approximately Correct)
framework for estimating the feasible reward set, provid-
ing the deﬁnition of pǫ,δq-PAC IRL algorithm. Then,
we investigate the relationships between several per-
formance indexes based on the Hausdorff metric (Sec-
tion4).
• We conceive, based on the provided PAC requirements in-
troduced, a novel sample complexity lower bound of or-
derΩ´
H3SA
ǫ2`
log`1
δ˘
`S˘¯
. This represents the most
signiﬁcant contribution and, to the best of our knowledge,
it is the ﬁrst lower bound that values the importance of
the relevant features of the IRL problem. From a tech-
nical perspective, the lower bound construction merges
new proof ideas with reworks of existing techniques (Sec-
tion5).
• We analyze a uniform sampling exploration strategy
(UniformSampling-IRL, US-IRL ) showing that, in the
generative model setting, it matches the lower bound up
to logarithmic factors (Section 6).
The complete proofs of the results presented in the main
paper are reported in Appendix B.
2. Preliminaries
In this section, we provide the background that will be em-
ployed in the subsequent sections.
Mathematical Background Leta,b PNwithaďb,
we denote with /llbracketa,b/rrbracket:“ ta,...,b uand with /llbracketa/rrbracket:“
/llbracket1,a/rrbracket. LetXbe a set, we denote with ∆Xthe set of
probability measures over X. LetYbe a set, we de-
note with ∆X
Ythe set of functions with signature YÑ
∆X. Let pX,dqbe a (pre)metric space, where Xis aset andd:XˆXÑ r0,`8s is a (pre)metric.2Let
Y,Y1ĎXbe non-empty sets, we deﬁne the Hausdorff
(pre)metric (Rockafellar & Wets ,1998 )Hd: 2Xˆ2XÑ
r0,`8s betweenYandY1induced by the (pre)metric das
follows:
HdpY,Y1q:“max"
sup
yPYinf
y1PY1dpy,y1q,sup
y1PY1inf
yPYdpy,y1q*
.(1)
Markov Decision Processes without Reward A time-
inhomogeneous ﬁnite-horizon Markov decision process
without reward (MDP\R) is deﬁned as a 4-tuple M“
pS,A,p,H qwhereSis a ﬁnite state space ( S“ |S|),A
is a ﬁnite action space ( A“ |A|),p“ pphqhP/llbracketH/rrbracketis the
transition model where for every stage hP/llbracketH/rrbracketwe have
phP∆S
SˆA, andHPNis the horizon. An MDP \R is
time-homogeneous if, for every stage hP/llbracketH´1/rrbracket, we have
ph“ph`1a.s.; in such a case, we denote the transition
model with the symbol ponly. A time-inhomogeneous re-
ward function is deﬁned as r“ prhqhP/llbracketH/rrbracket, where for every
stagehP/llbracketH/rrbracketwe haverh:SˆAÑ r´1,1s.3AMarkov
decision process (MDP, Puterman ,1994 ) is obtained by
pairing an MDP \RMwith a reward function r. The
agent’s behavior is modeled with a time-inhomogeneous
policyπ“ pπhqhP/llbracketH/rrbracketwhere for every stage hP/llbracketH/rrbracket,
we have πhP∆A
S. LetfPRSandgPRSˆA, we
denote with phfps,aq “ř
s1PSphps1|s,aqfps1qand with
πhgpsq “ř
aPAπhpa|sqgps,aqthe expectation operators
w.r.t. the transition model and the policy, respectively.
Value Functions and Optimality Given an MDP \R
M, a policy π, and a reward function r, the Q-function
Qπp¨;rq “ pQπ
hp¨;rqqhP/llbracketH/rrbracketinduced by rrepresents the ex-
pected sum of rewards collected starting from ps,a,h q P
SˆAˆ/llbracketH/rrbracketand following policy πthereafter:
Qπ
hps,a;rq:“E
pM,πq«Hÿ
l“hrlpsl,alq|sh“s,ah“aﬀ
,
whereEpM,πqdenotes the expectation w.r.t. Mandπ, i.e.,
ah„πhp¨|shqandsh`1„php¨|sh,ahqfor every stage
hP/llbracketh,H/rrbracket. The Q-function fulﬁlls the Bellman equa-
tions ( Puterman ,1994 ) for every ps,a,h q PSˆAˆ/llbracketH/rrbracket:
Qπ
hps,a;rq “rhps,aq `phVπ
h`1ps,a;rq,
Vπ
hps;rq “πhQπ
hps;rqandVπ
H`1ps;rq “0,
whereVπp¨;rq “ pVπ
hp¨;rqqhP/llbracketH/rrbracketis the V-function . The
advantage function Aπ
hps,a;rq “Qπ
hps,a;rq ´Vπ
hps;rq
represents the relative gain of playing action aPArather
than following policy πin the state-stage pair ps,hq. A
policyπ˚isoptimal if it has non-positive advantage ev-
2Apremetric dsatisﬁes the axioms: dpx,x1q ě0and
dpx,xq “0for allx,x1PX. Any metric is clearly a premet-
ric.
3For the sake of simplicity and w.l.o.g., we restrict to rewar d
functions bounded by 1in absolute value.Towards Theoretical Understanding of Inverse Reinforceme nt Learning
erywhere, i.e., Aπ˚
hps,a;rq ď0for every ps,a,h q P
SˆAˆ/llbracketH/rrbracket. The Q- and V-functions of an optimal policy
are denoted with Q˚
hps,a;rqandV˚
hps;rq.
Inverse Reinforcement Learning Aninverse reinforce-
ment learning problem (IRL, Ng & Russell ,2000 ) is de-
ﬁned as a pair pM,πEq, whereMis an MDP \R andπE
is an expert’s policy . Informally, solving an IRL problem
consists in ﬁnding a reward function prhqhP/llbracketH/rrbracketmakingπE
optimal for the MDP \RMpaired with reward function r.
Any reward function fulﬁlling this condition is called fea-
sible and the set of all such reward functions is called fea-
sible reward set (Metelli et al. ,2021 ;Lindner et al. ,2022 ),
deﬁned as:
RpM,πEq:“!
prhqhP/llbracketH/rrbracket/vextendsingle/vextendsingle/vextendsingle@hP/llbracketH/rrbracket:rh:SˆAÑr´1,1s
^@ps,a,h qPSˆAˆ/llbracketH/rrbracket:AπE
hps,a;rqď0)
.(2)
We will omit the subscript pM,πEqwhenever clear from
the context.
Empirical MDP and Empirical Expert’s Policy Let
D“tpsl,al,hl,s1
l,aE
lqulP/llbrackett/rrbracketbe a dataset of tPNtuples,
where for every lP/llbrackett/rrbracket, we have s1
l„phlp¨|sl,alqandaE
l„
πE
hlp¨|slq. We introduce the counts for every ps,a,h qPSˆ
Aˆ/llbracketH/rrbracket:nt
hps,a,s1q:“řt
l“11tpsl,al,hl,s1
lq“ps,a,h,s1qu,
nt
hps,aq:“ř
s1PSnt
hps,a,s1q,nt
hpsq:“ř
aPAnt
hps,aq, and
nt,E
hps,aq:“řt
l“11tpsl,aE
lq“ps,aqu. These quantities al-
low deﬁning the empirical transition model ppt“pppt
hqhP/llbracketH/rrbracket
and empirical expert’s policy pπt,E“pπt,E
hqhP/llbracketH/rrbracketas fol-
lows:
ppt
hps1|s,aq:“#nt
hps,a,s1q
nt
hps,aqifnt
hps,aq ą0
1
Sotherwise,
pπE,t
hpa|sq:“#nE,t
hps,aq
nt
hpsqifnt
hpsq ą0
1
Aotherwise.(3)
In the time-homogeneous case, we simply merge the sam-
ples collected at different stages hP/llbracketH/rrbracket. We denote
with pxMt,pπE,tqtheempirical IRL problem, where xMt“
pS,A,ppt,Hqthe empirical MDP \R induced by ppt. Finally,
we denote with pRt:“RpxMt,pπE,tqthe feasible reward set
induced pxMt,pπE,tq. We will omit the superscript t, when-
ever clear from the context and write pR.
3. Lipschitz Framework for IRL
In this section, we analyze the regularity properties of the
feasible reward set in terms of the Lipschitz continuity w.r .t.
the IRL problem. To make the idea more concrete, sup-
pose that Ris the feasible reward set obtained from the
IRL problem pM,πEqand thatpRis obtained with a dif-
ferent IRL problem pxM,pπEq, which we can think to as an
empirical version of pM,πEq, with an estimated transitionmodelppreplacing the true model p. Intuitively, to have any
learning guarantee, “similar” IRL problems ( p«ppand
πE«pπE) should lead to “similar” feasible reward sets
(R«pR).4
To formally deﬁne a Lipschitz framework, we need to se-
lect a (pre)metric for evaluating dissimilarities between fea-
sible reward sets and IRL problems. While we defer the
presentation of the (pre)metric for the IRL problems to Sec-
tion3.1, where it will emerge naturally, for the feasible re-
ward sets, we employ the Hausdorff (pre)metric HdpR,pRq
(Equation 1), induced by a (pre)metric dpr,prqused to eval-
uate the dissimilarity between individual reward function s
rPRandprPpR. With this choice, two feasible reward
sets are “similar” if every reward rPRis “similar” to some
rewardprPpRin terms of the (pre)metric d. In the next sec-
tions, we employ as dthe metric induced by the L8-norm
between the reward functions rPRandprPpR:5
dGpr,prq:“max
ps,a,h qPSˆAˆ/llbracketH/rrbracket|rhps,aq ´prhps,aq|,(4)
where G stands for “generative”. In Section 3.1, we prove
that the Lipschitz continuity is fulﬁlled when no restricti ons
on the reward function are enforced (besides boundedness
inr´1,1s). Then, in Section 3.2, we show that, when fur-
ther restrictions on the viable rewards are required (e.g.,
state-only reward), such a regularity property no longer
holds.
3.1. Lipschitz Continuous Feasible Reward Sets
In order to prove the Lipschitz continuity property, we use
theexplicit form of the feasible reward sets introduced
in (Metelli et al. ,2021 ) and extended by ( Lindner et al. ,
2022 ) for the ﬁnite-horizon case, that we report below.
Lemma 3.1 (Lemma 4 of Lindner et al. (2022 )).A re-
ward function r“ prhqhP/llbracketH/rrbracketis feasible for the IRL
problem pM,πEqif and only if there exist two functions
pAh,VhqhP/llbracketH/rrbracketwhere for every hP/llbracketH/rrbracketwe have Ah:
SˆAÑRě0,Vh:SˆAÑR, andVH`1“0, such
that for every ps,a,h q PSˆAˆ/llbracketH/rrbracketit holds that:
rhps,aq“´Ahps,aq1tπE
hpa|sq“0u`Vhpsq´phVh`1ps,aq.
Furthermore, if |rhps,aq| ď1, if follows that |Vhpsq| ď
H´h`1andAhps,aq ďH´h`1.
A form of regularity of the feasible reward set was al-
ready studied in Theorem of 3.1 of Metelli et al. (2021 ) and
in Theorem 5 of Lindner et al. (2022 ), providing an error
propagation analysis. These results are based on showing
the existence of a particular rewardrrfeasible for the IRL
4If not, any arbitrary accurate estimate ppp,pπEqofpp,πEq,
may induce feasible sets pRandRwith ﬁnite non-zero dissimi-
larity.
5We discuss other choices of din Section 4.Towards Theoretical Understanding of Inverse Reinforceme nt Learning
problem pxM,pπEq, whose distance from the original reward
functionrPRis bounded by a dissimilarity term between
pM,πEqandpxM,pπEq. Unfortunately, such a reward rr
is not guaranteed to be bounded in r´1,1seven when the
original reward ris (and, thus, it might be rrRpRaccording
to Equation 2).6In Lemma B.1, with a modiﬁed construc-
tion, we show the existence of another particular feasible
rewardprbounded in r´1,1s(and, thus, prPpR). From this,
the Lipschitz continuity of the feasible reward sets follow s.
Theorem 3.2 (Lipschitz Continuity) .LetRandpRbe the
feasible reward sets of the IRL problems pM,πEqand
pxM,pπEq. Then, it holds that:7
HdGpR,pRq ď2ρGppM,πEq,pxM,pπEqq
1`ρGppM,πEq,pxM,pπEqq, (5)
whereρGp¨,¨qis a (pre)metric between IRL problems, de-
ﬁned as:
ρGppM,πEq,pxM,pπEqq:“max
ps,a,h qPSˆAˆ/llbracketH/rrbracketpH´h`1q
ˆ´ˇˇˇ1tπE
hpa|sq“0u´1tpπE
hpa|sq“0uˇˇˇ`}php¨|s,aq´pphp¨|s,aq}1¯
.
Some observations are in order. First, the function ρGis in-
deed a (pre)metric since it is non-negative and takes value
0when the IRL problems coincide. Second, as supported
by intuition, ρGis composed of two terms related to the es-
timation of the expert’s policy and of the transition model.
While for the transition model, the dissimilarity is formal -
ized by the L1-norm distance }php¨|s,aq ´pphp¨|s,aq}1, for
the policy, the resulting term deserves some comments. In-
deed, the dissimilarity |1tπE
hpa|sq“0u´1tpπE
hpa|sq“0u|high-
lights that what matters is whether an action aPAis
played by the expert and not the corresponding probability
πE
hpa|sq. Indeed, the expert’s policy plays an action (with
any non-zero probability) only if it is an optimal action.
3.2. Non-Lipschitz Continuous Feasible Reward Sets
In this section, we illustrate three cases of feasible rewar d
sets restrictions that turn out not to fulﬁll the condition o f
Theorem 3.2. These examples consider three conditions
commonly enforced in the literature: state-only reward
functionrhpsq(Example 3.1), time-homogeneous reward
functionrps,aq(Example 3.2), andβ-margin reward func-
tion (Example 3.3). We present counter-examples in which
in front of ǫ-close transition models, the induced feasible
sets are far apart by a constant independent of ǫ. For space
reasons, we report the complete derivation in Appendix C.
Example 3.1 (State-only reward rhpsq).State-only reward
functions have been widely considered in many IRL ap-
6We illustrate in Fact B.1an example of this phenomenon.
7This implies the standard Lipschitz continuity, by simply
bounding2ρGppM,πEq,pxM,pπEqq
1`ρGppM,πEq,pxM,pπEqqď2ρGppM,πEq,pxM,pπEqq.s0
s´s`
a1a21{2
1{2
11
(a)
s0 s1
a1a21{2
1{2
1
(b)
Figure 1. The MDP \R employed in the examples of Section 3.2.
denotes a transition executed for multiple actions.
proaches (e.g., Ng & Russell ,2000 ;Abbeel & Ng ,2004 ;
Syed & Schapire ,2007 ;Komanduru & Honorio ,2019 ).
We formalize the state-only feasible reward set as follows:
Rstate“RX t@ps,a,a1,hq:rhps,aq “rhps,a1qu.
Consider the MDP \R of Figure 1awithH“2,πE
hps0q“
pπE
hps0q“a1withhPt1,2u. Setp1ps`|s0,a1q“1{2`ǫ{4
andpp1ps`|s0,a1q“1{2´ǫ{4and, thus, }p1p¨|s0,a1q´
pp1p¨|s0,a1q}1“ǫ. Let us set r2ps`q“1andr2ps´q“´1,
which makes πEoptimal under p. We observe that pRis
deﬁned by pr2ps´qďpr2ps`q. Recalling that the rewards are
bounded in r´1,1s, we have HdGpRstate,pRstateqě1.
Example 3.2 (Time-homogeneous reward rps,aq).Time-
homogeneous reward functions have been employed in
several RL (e.g., Dann & Brunskill ,2015 ) and IRL set-
tings (e.g., Lindner et al. ,2022 ). We formalize the time-
homogeneous feasible reward set as follows:
Rhom“RX t@ps,a,h,h1q:rhps,aq “rh1ps,aqu.
Consider the MDP \R of Figure 1bwithH“2,πE
1ps0q“
pπE
1ps0q“a1andπE
2ps0q“pπE
2ps0q“a2. ForhPt1,2u, we
setphps0|s0,a1q“1{2`ǫ{4andpphps0|s0,a1q“1{2´ǫ{4,
thus, }php¨|s0,a1q´pphp¨|s0,a1q}1“ǫ. We setrps0,a1q“1,
rps0,a2q“1´ǫ{6, andrps1,a1q“rps1,a2q“1{2making
πEoptimal. We can prove that HdGpRhom,pRhomqě1{4.
Example 3.3 (β-margin reward) .Aβ-margin reward en-
forces a suboptimality gap of at least βą0(Ng & Russell ,
2000 ;Komanduru & Honorio ,2019 ). We formalize it in
the ﬁnite-horizon case with a sequence β“ pβhqhP/llbracketH/rrbracket,
possibly different for every stage:
Rβ-mar“RXt@ps,a,h q:AπE
hps,a;rqPt0uYp´8,´βhsu.Towards Theoretical Understanding of Inverse Reinforceme nt Learning
Consider the MDP \R in Figure 1awithπE
hps0q “
pπE
hps0q “a1forhP t1,2u. We setp1ps`|s0,a1q “1{2`ǫ
andpp1ps`|s0,a1q “1{2´ǫ. We set for MDP \RM
the reward function as r1ps0,aq “0andrhps`,aq “
´rhps´,aq “1foraP ta1,a2uandhP/llbracket2,H/rrbracket. Inps0,1q
the suboptimality gap is β1“2`2ǫpH´1q. By selecting
Hě1`1{ǫ, the feasible set pRβ-maris empty.
These examples show that, under certain classes of restric-
tions, the feasible reward set is not Lipschitz continuous
w.r.t. the transition model and, more in general, w.r.t. the
IRL problem. The generalization of these examples to
more abstract conditions for guaranteeing the Lipschitz
continuity of the feasible reward set is beyond the scope
of the paper.
4. PAC Framework for IRL with a Generative
Model
In this section, we discuss the PAC (Probably Approxi-
mately Correct) requirements for estimating the feasible r e-
ward set with access to a generative model of the environ-
ment. We ﬁrst provide the notion of a learning algorithm
estimating the feasible reward set with a generative model
(Section 4.1). Then, we formally present the PAC require-
ment for the Hausdorff (pre)metric Hd(Section 4.2). Fi-
nally, we discuss the relationships between the PAC require -
ments with different choices of (pre)metric d(Section 4.3).
4.1. Learning Algorithms with a Generative Model
A learning algorithm for estimating the feasible reward set
is a pairA“ pµ,τq, whereµ“ pµtqtPNis asampling strat-
egydeﬁned for every time step tPNasµtP∆SˆAˆ/llbracketH/rrbracket
Dt´1
withDt“ pSˆAˆ/llbracketH/rrbracketˆSˆAqtandτis a stop-
ping time w.r.t. a suitably deﬁned ﬁltration. At every step
tPN, the learning algorithm query the environment in a
triple pst,at,htq, selected based on the sampling strategy
µtp¨|Dt´1q, whereDt´1“ ppsl,al,hl,s1
l,aE
lqqt´1
l“1PDt´1
is the dataset of past samples. Then, the algorithm ob-
serves the next state s1
t„phtp¨|st,atqand expert’s action
aE
t„πE
htp¨|stqand updates the dataset Dt“Dt´1‘
pst,at,ht,s1
t,aE
tq. Based on the collected data Dτ, the al-
gorithm computes the empirical IRL problem pxMτ,pπE,τq,
based on Equation ( 3) and the empirical feasible reward set
pRτ.
4.2. PAC Requirement
We now introduce a general notion of a PAC requirement
for estimating the feasible reward set of an IRL prob-
lem. To this end, we consider the Hausdorff (pre)metric
introduced in Section 3deﬁned in terms of the reward
(pre)metric dpr,prq. We denote with d-IRL the problemof estimating the feasible reward set under the Hausdorff
(pre)metric Hd.
Deﬁnition 4.1 (PAC Algorithm for d-IRL ).LetǫP p0,2q
andδP p0,1q. An algorithm A“ pµ,τqispǫ,δq-PAC for
d-IRL if:
P
pM,πEq,A´
HdpR,pRτq ďǫ¯
ě1´δ,
wherePpM,πEq,Adenotes the probability measure induced
by executing the algorithm Ain the IRL problem pM,πEq
andpRτis the feasible reward set induced by the empirical
IRL problem pxMτ,pπE,τqestimated with the dataset Dτ.
Thesample complexity is deﬁned as τ:“ |Dτ|.
In the next section, we show the relationship between PAC
requirements deﬁned for notable choices of d.
4.3. Different Choices of d
So far, we have evaluated the dissimilarity between the fea-
sible reward sets by means of the Hausdorff induced by
dG, i.e., the L8-norm of between individual reward func-
tions. In the literature, other (pre)metrics dhave been pro-
posed (e.g., Metelli et al. ,2021 ;Lindner et al. ,2022 ).
dG
Q˚-IRL Since the recovered reward functions are often
used for performing forward RL, an index of interest is the
dissimilarity between optimal Q-functions obtained with
the reward rPRandprPpRin the original MDP \R:
dG
Q˚pr,prq:“max
ps,a,h qPSˆAˆ/llbracketH/rrbracket|Q˚
hps,a;rq ´Q˚
hps,a;prq|.
dG
V˚-IRL We are often interested in not just being accu-
rate in estimating the optimal Q-function, but rather in the
performance of an optimal policy pπ˚, learned with the re-
covered reward prPpR, evaluated under the true reward
rPR:
dG
V˚pr,prq:“sup
pπ˚PΠ˚pprqmax
ps,hqPSˆ/llbracketH/rrbracketˇˇˇV˚
hps;rq´Vpπ˚
hps;rqˇˇˇ,
whereΠ˚pprq:“tπ:@ps,a,h qPSˆAˆ/llbracketH/rrbracket:Aπ
hps,a;prqď0u
is the set of optimal policies under the recovered reward pr.
The following result formalizes the relationships between
the presented d-IRL problems.
Theorem 4.1 (Relationships between d-IRL problems) .
Let us introduce the graphical convention for cą0:
x-IRL y-IRLc
meaning that any pǫ,δq-PACx-IRL algorithm is pcǫ,δq-
PACy-IRL. Then, the following statements hold:Towards Theoretical Understanding of Inverse Reinforceme nt Learning
dG-IRL dG
Q˚-IRL dG
V˚-IRL .2H
H 2H
Theorem 4.1shows that any pǫ,δq-PAC guarantee on dG,
implies pǫ1,δq-PAC guarantees on both dG
Q˚anddG
V˚,
whereǫ1“ΘpHǫqis linear in the horizon H. This jus-
tiﬁes why focusing on dG-IRL, as in the following section
where sample complexity lower bounds are derived. The
lower bound analysis for dG
Q˚-IRL and dG
V˚-IRL is left to
future works.
5. Lower Bounds
In this section, we establish sample complexity lower
bounds for the dG-IRL problem based on the PAC require-
ment of Deﬁnition 4.1in the generative model setting. We
start presenting the general result (Section 5.1) and, then,
we comment on its form and, subsequently, provide a
sketch of the construction of the hard instances for obtain-
ing the lower bound (Section 5.2). For the sake of presen-
tation, we assume that the expert’s policy πEis known; the
extension to the case of unknown πEis reported in Ap-
pendix D.
5.1. Main Result
In this section, we report the main result of the lower bound
of the sample complexity of learning the feasible reward
set.
Theorem 5.1 (Lower Bound for dG-IRL) .LetA“ pµ,τq
be an pǫ,δq-PAC algorithm for dG-IRL. Then, there exists
an IRL problem pM,πEqsuch that, if δď1{32,Sě9,
Aě2, andHě12, the expected sample complexity is
lower bounded by:
•if the transition model pis time-inhomogeneous:
E
pM,πEq,Arτs ě1
1024H3SA
ǫ2ˆ1
2logˆ1
δ˙
`1
5S˙
;
•if the transition model pis time-homogeneous:
E
pM,πEq,Arτs ě1
512H2SA
ǫ2ˆ1
2logˆ1
δ˙
`1
5S˙
,
whereEpM,πEq,Adenotes the expectation w.r.t. the proba-
bility measure PpM,πEq,A.
Some observations are in order. First, the derived lower
bound displays a linear dependence on the number of ac-
tionsAand dependence on the horizon Hraised to a power
2or3, which depends on whether the underlying transition
model is time-homogeneous, as common even for forward
RL (e.g., Dann & Brunskill ,2015 ;Domingues et al. ,2021 ).
Second, we identify two different regimes visible insidethe parenthesis related to the dependence on the number of
statesSand the conﬁdence δ. Speciﬁcally, for small values
ofδ(i.e.,δ«0), the dominating part is log`1
δ˘
, leading to
a sample complexity of order Ω´
H3SA
ǫ2log`1
δ˘¯
. Instead,
for large δ(i.e.,δ«1{32), the most relevant part is the
one corresponding to S, leading to sample complexity of
orderΩ´
H3S2A
ǫ2¯
(both for the time-inhomogeneous case).
An analogous two-regime behavior has been previously
observed in the reward-free exploration setting ( Jin et al. ,
2020 ;Kaufmann et al. ,2021 ;M´ enard et al. ,2021 ).
5.2. Sketch of the Proof
In this section, we provide a sketch of the construction
of the lower bounds of Theorem 5.1. The idea consists
in deriving two separate bounds depending on the regime
ofδ, which are based on two building blocks reported
in Figure 2. These instances are used to build lower
bounds for a single state s˚and the extension to multi-
ple states and stages follows standard constructions (e.g. ,
Domingues et al. ,2021 ).
Small-δregime Figure 2areports the instances employed
in this regime. The expert’s policy is πEpsq “a0. From
states˚, all actions bring the system to the absorbing
statess`ands´with equal probability, except for action
a˚‰a0that increases by ǫ1ą0the probability of reach-
ing state s`. The learner, in order to recover a correct
feasible reward set, has to identify which is the action be-
having like a˚(among the Aavailable ones) to force ac-
tiona0to be optimal. Considering ΘpAqinstances, in
which action a˚changes, an application of Bretagnolle-
Huber inequality (Lattimore & Szepesv´ ari ,2020 , Theorem
14.2) allows deriving a sample complexity lower bounded
byΩ´
AH2
ǫ2log`1
δ˘¯
.
Large-δregime Figure 2bdepicts the instances used in
this regime. The expert’s policy is again πEpsq “a0. The
system, instead, is made of S“ΘpSqnext states reachable
with equal probability by playing action a0. All other ac-
tionsaj‰a0alter the probability distribution of the next
state. Speciﬁcally, by playing the action aj‰a0, the proba-
bility of reaching the next state s1
kis given by p1`ǫ1vpjq
kq{S,
wherevpjqP t´1,1uSis a vector such thatřS
k“1vpjq
k“0.
By varying vjin a suitable set, deﬁned by means of a pack-
ing argument, we obtain Θp2Sqinstances each one sepa-
rated by a ﬁnite dissimilarity, depending on ǫ1. We obtain
the lower bound by means of an application of the Fano’s
inequality (Gerchinovitz et al. ,2017 , Proposition 4) which
results in order Ω´
pp1´δq´log2 qS2AH2
ǫ2¯
.
Extension to Multiple States and Stages At the begin-
ning, the system randomly chooses a problem between Fig-Towards Theoretical Understanding of Inverse Reinforceme nt Learning
s˚
s´s`
a˚‰a˚
1{2´ǫ11{2
1{2`ǫ1
1{2
11
(a) MDP\R used for the small- δregime.s˚
sS...s1
s2
a0aj‰a01{2
1{2
1{2p1`ǫ1vSq{Sp1`ǫ1v1q{S
p1`ǫ1v2q{S
11
1
(b) MDP\R used for the large- δregime.
Figure 2. The MDP \R employed in the constructions of the lower bounds of Sectio n5. The expert’s policy is πEpsq “a0.
denotes a transition executed for multiple actions.
Input: signiﬁcance δP p0,1q,ǫtarget accuracy
tÐ0,ǫ0Ð `8
whileǫtąǫdo
tÐt`SAH
Collect one sample from each ps,a,h q PSˆAˆ/llbracketH/rrbracket
Updatepptaccording with ( 3)
Updateǫt“max ps,a,h qPSˆAˆ/llbracketH/rrbracketCt
hps,aq(resp.rCt
hps,aq)
end while
Algorithm 1. UniformSampling-IRL ( US-IRL ) for time-
inhomogeneous (resp. time-homogeneous ) transition models.
ure2aand Figure 2b. Then, it transitions to the state in
which the system may randomly remain for HăHstages
after which it transitions with uniform probability to any o f
theΘpSqstates.H“ΘpHqfor the time-inhomogeneous
(resp.H“Op1qfor the time-homogeneous) case. In
any state s˚and stage h˚, the agent can face the problems
shown in Figure 2. By varying s˚andh˚among its possi-
bleHS(resp.S) values, we get the bounds in Theorem 5.1.
Remark 5.1 (Generative vs Forward models) .This con-
struction sufﬁces for obtaining a bound for the generative
model, but it can be easily extended to work with the for-
ward model of the environment (in which the agent interacts
via trajectories only) by means of a standard tree-based
construction (Jin et al. ,2020 ;Domingues et al. ,2021 ). In
such a case, the resulting PAC guarantee would no longer
be expressed via the L8-norm distance dGbetween reward,
butworst-case over the visitation distributions induced by
the policies: dFpr,prq:“supπEM,πr|rhps,aq ´prhps,aq|s.
6. Algorithm
In this section, we analyze the sample complexity of a uni-
form sampling strategy (UniformSampling-IRL, US-IRL )
for thedG-IRL problem (Algorithm 1). We start presenting
the sample complexity analysis (Section 6.1) and, then, we
provide a sketch of the proof (Section 6.2).6.1. Main Result
TheUS-IRL algorithm was presented in ( Metelli et al. ,
2021 ;Lindner et al. ,2022 ) but analyzed for different IRL
formulations (see Section 7). We revise it since it matches
our sample complexity lower bounds, provided that more
sophisticated concentration tools w.r.t. those employed
in (Metelli et al. ,2021 ;Lindner et al. ,2022 ). For the sake
of presentation, we assume that the expert’s policy πEis
known; the extension to unknown πEis reported in Ap-
pendix D. At each iteration, the algorithm collects a sam-
ple from every ps,a,h q PSˆAˆ/llbracketH/rrbracketand, for time-
inhomogeneous models, computes the conﬁdence function:
Ct
hps,aq:“2?
2pH´h`1qd
2β`
nt
hps,aq,δ˘
nt
hps,aq,(6)
whereβ`
n,δ˘:“logpSAH {δq` pS´1qlog`
ep1`n{pS´
1q˘
.8The algorithm stops as soon as all conﬁdence func-
tions fall below the threshold ǫ. The following theorem
provides the sample complexity of US-IRL .
Theorem 6.1 (Sample Complexity of US-IRL ).Letǫą0
andδP p0,1q,US-IRL ispǫ,δq-PAC for dG-IRL and with
probability at least 1´δit stops after τsamples with:
•if the transition model pis time-inhomogeneous:
τď8H3SA
ǫ2ˆ
logˆSAH
δ˙
` pS´1qC˙
,
whereC “logpe{pS´1q ` p8eH2q{ppS´
1qǫ2qplogpSAH {δq `4eqq;
8In the time-homogeneous case, the algorithm merges the sam-
ples collected at different hP/llbracketH/rrbracketfor the estimation of the tran-
sition model and replaces the conﬁdence function with:
rCt
hps,aq:“2?
2pH´h`1qd
2rβ`
ntps,aq,δ˘
ntps,aq, (7)
whererβ`
n,δ˘:“logpSA{δq ` pS´1qlog`
ep1`n{pS´1q˘
andntps,aq “řH
h“1nt
hps,aq.Towards Theoretical Understanding of Inverse Reinforceme nt Learning
•if the transition model pis time-homogeneous and :
τď8H2SA
ǫ2ˆ
logˆSA
δ˙
` pS´1qC2˙
,
whererC “logpe{pS´1q ` p8eH2q{ppS´
1qǫ2qplogpSA{δq `4eqq.
Thus, time-inhomogeneous (resp. time-homogeneous)
transition models, US-IRL suffers a sample com-
plexity bound of order rO´
H3SA
ǫ2`
log`1
δ˘
`S˘¯
(resp.
rO´
H2SA
ǫ2`
log`1
δ˘
`S˘¯
) matching the lower bounds of
Theorem 5.1up to logarithmic factors for both regimes of
δ.
6.2. Sketch of the Proof
The idea of the proof is to exploit Theorem 3.2to reduce
the Hausdorff distance to the L1-norm between the tran-
sition model }ppt
hp¨|s,aq ´php¨|s,aq}1. It is worth not-
ing this term replaces |pppt
h´phqVh|appearing in previ-
ous works ( Metelli et al. ,2021 ;Lindner et al. ,2022 ) that
was comfortably bounded using H¨ oeffding’s inequality. In
our case, the L1-norm is unavoidable due to the Haus-
dorff distance that implies a worst-case choice of the re-
ward function and, thus, of Vh. This term has to be care-
fully bounded using the stronger KL-divergence concentra-
tion result of ( Jonsson et al. ,2020 , Proposition 1) to get the
Oplogp1{δq `Sqrate.9
7. Related Works
In this section, we discuss the related works about sample
complexity analysis and lower bounds for IRL. Additional
related works are reported in Appendix A.
Sample Complexity for Estimating the Feasible Reward
Set The notion of feasible reward set Rwas intro-
duced in ( Ng & Russell ,2000 ) in an implicit form in the
inﬁnite-horizon discounted case as a linear feasibility prob-
lem and, subsequently, adapted to the ﬁnite-horizon case
in (Lindner et al. ,2022 ). Furthermore, in ( Metelli et al. ,
2021 ;Lindner et al. ,2022 ) an explicit form of the reward
functions belonging to the feasible region Rwas provided.
In these works, the problem of estimating the feasible re-
ward set is studied for the ﬁrst time considering a “refer-
ence” pair of rewards pr,qrq PRˆpRagainst which to
compare the rewards inside the recovered sets, leading to
the (pre)metric:
rHdpR,R,r,qrq:“max"
inf
prPpRdpr,prq,inf
rPRdpr,qrq*
.(8)
9A more na¨ ıve application of the L1-concentration
of ( Weissman et al. ,2003 ) would lead to the worse
OpSlogp1{δqqrate.Compared to the Hausdorff (pre)metric (Equation 1), in
Equation ( 8) there is no maximization over the choice of
pr,qrq, leading to a simpler problem.10In (Metelli et al. ,
2021 ), a uniform sampling approach (similar to Algo-
rithm 1) is proved to achieve a sample complexity of order
rO´
γ2SA
p1´γq4ǫ2¯
for the index of Equation ( 8) withd“dG
Q˚
in the discounted setting with generative model. For the
forward model case, the AceIRL algorithm ( Lindner et al. ,
2022 ) suffers a sample complexity of order rO´
H5SA
ǫ2¯
for
the index of Equation ( 8) withd“dF
V˚, in the ﬁnite-
horizon case.11Unfortunately, the reward recovered by
AceIRL reward function is not guaranteed to be bounded
by a predetermined constant (e.g., r´1,1s). Modiﬁed
versions of these algorithms allow embedding problem-
dependent features under a speciﬁc choice of a reward
within the set.
Sample Complexity Lower Bounds in IRL To the best of
our knowledge, the only work that proposes a sample com-
plexity lower bound for IRL is ( Komanduru & Honorio ,
2021 ). The authors consider a ﬁnite state and action
MDP\R and the IRL algorithm of ( Ng & Russell ,2000 ) for
β-strict separable IRL problems (i.e., with suboptimality
gap at least β) with state-only rewards in the discounted set-
ting. When only two actions are available ( A“2) and the
samples are collected starting in each state with equal prob -
ability, by means of a geometric construction and Fano’s
inequality, the authors derive an ΩpSlogSqlower bound
on the number of trajectories needed to identify a reward
function. Note that this analysis limits to the identiﬁcation
of a reward function within a ﬁnite set, rather than evaluat-
ing the accuracy of recovering the feasible reward set.
8. Conclusions and Open Questions
In this paper, we provided contributions to the understand-
ing of the complexity of the IRL problem. We conceived
a lower bound of order Ω´
H3SA
ǫ2`
log`1
δ˘
`S˘¯
on the
number samples collected with a generative model in the
ﬁnite-horizon setting. This result is of relevant interest
since it sets, for the ﬁrst time, the complexity of the IRL
problem, deﬁned as the problem of estimating the feasible
reward set. Furthermore, we showed that a uniform sam-
pling strategy matches the lower bound up to logarithmic
factors. Nevertheless, the IRL problem is far from being
closed. In the following, we outline a road map of open
questions, hoping to inspire researchers to work in this ap-
pealing area.
10In this sense, a PAC guarantee according to Deﬁnition 4.1,
implies a PAC guarantee deﬁned w.r.t. (pre)metric of Equati on (8).
11As discussed in Remark 5.1, in the forward model case, the
dissimilarity is in expectation w.r.t. the worst-case poli cy.Towards Theoretical Understanding of Inverse Reinforceme nt Learning
Forward Model The most straightforward extension of
our ﬁndings is moving to the forward model setting, in
which the agent can interact with the environment through
trajectories only. As we already noted, our lower bounds
can be comfortably extended to this setting. However, in
this case, the PAC requirement has to be relaxed since con-
trolling the L8-norm between rewards is no longer a vi-
able option (e.g., for the possible presence of almost un-
reachable states). Which distance notion should be used
for this setting? Will the Lipschitz regularity of Section 3
still hold?
Problem-Dependent Analysis Our analysis is worst-case
in the class of IRL problems. Would it be possible to
obtain a problem-dependent complexity results? Previous
problem-dependent analyses provided results tightly con-
nected to the properties of the speciﬁc reward selection pro -
cedure ( Metelli et al. ,2021 ;Lindner et al. ,2022 ). Clearly,
a currently open question, in all settings in which reward is
missing, including reward-free exploration ( Jin et al. ,2020 )
and IRL, is how to deﬁne a problem-dependent quantity in
replacement of the suboptimality gaps.
Reward Selection Our PAC guarantees concern with the
complete feasible reward set. However, algorithmic solu-
tions to IRL implement a speciﬁc criterion for selecting a
reward (e.g., maximum entropy, maximum margin). How
the PAC guarantee based on the Hausdorff distance relates
to guarantees on a single reward selected with a speciﬁc
criterion withinR?
References
Abbeel, P. and Ng, A. Y . Apprenticeship learning via
inverse reinforcement learning. In Proceedings of the
Twenty-ﬁrst International Conference on Machine Learn-
ing (ICML) , volume 69 of ACM International Confer-
ence Proceeding Series . ACM, 2004.
Adams, S. C., Cody, T., and Beling, P. A. A survey of
inverse reinforcement learning. Artif. Intell. Rev. , 55(6):
4307–4346, 2022.
Arora, S. and Doshi, P. A survey of inverse reinforcement
learning: Challenges, methods and progress. Artif. In-
tell., 297:103500, 2021.
Cohen, G. D. and Frankl, P. Good coverings of hamming
spaces with spheres. Discret. Math. , 56(2-3):125–131,
1985.
Dann, C. and Brunskill, E. Sample complexity of episodic
ﬁxed-horizon reinforcement learning, 2015.
Dexter, G., Bello, K., and Honorio, J. Inverse reinforce-
ment learning in a continuous state space with formalguarantees. In Advances in Neural Information Process-
ing Systems 34 (NeurIPS) , pp. 6972–6982, 2021.
Domingues, O. D., M´ enard, P., Kaufmann, E., and Valko,
M. Episodic reinforcement learning in ﬁnite mdps: Min-
imax lower bounds revisited. In Algorithmic Learning
Theory (ALT) , volume 132 of Proceedings of Machine
Learning Research , pp. 578–598. PMLR, 2021.
Gerchinovitz, S., M´ enard, P., and Stoltz, G. Fano’s inequa l-
ity for random variables. CoRR , abs/1702.05985, 2017.
Gy¨ orﬁ, L., Kohler, M., Krzyzak, A., and Walk, H. A
Distribution-Free Theory of Nonparametric Regression .
Springer series in statistics. Springer, 2002.
Jin, C., Krishnamurthy, A., Simchowitz, M., and Yu, T.
Reward-free exploration for reinforcement learning. In
Proceedings of the 37th International Conference on
Machine Learning (ICML) , volume 119 of Proceedings
of Machine Learning Research , pp. 4870–4879. PMLR,
2020.
Jonsson, A., Kaufmann, E., M´ enard, P., Domingues, O. D.,
Leurent, E., and Valko, M. Planning in markov deci-
sion processes with gap-dependent sample complexity.
InAdvances in Neural Information Processing Systems
33 (NeurIPS) , 2020.
Kaufmann, E., M´ enard, P., Domingues, O. D., Jonsson, A.,
Leurent, E., and Valko, M. Adaptive reward-free explo-
ration. In Algorithmic Learning Theory (ALT) , volume
132 of Proceedings of Machine Learning Research , pp.
865–891. PMLR, 2021.
Komanduru, A. and Honorio, J. On the correctness and
sample complexity of inverse reinforcement learning. pp.
7110–7119, 2019.
Komanduru, A. and Honorio, J. A lower bound for the
sample complexity of inverse reinforcement learning. In
Proceedings of the 38th International Conference on
Machine Learning (ICML) , volume 139 of Proceedings
of Machine Learning Research , pp. 5676–5685. PMLR,
2021.
Lattimore, T. and Szepesv´ ari, C. Bandit algorithms . Cam-
bridge University Press, 2020.
Lindner, D., Krause, A., and Ramponi, G. Active ex-
ploration for inverse reinforcement learning. CoRR ,
abs/2207.08645, 2022.
M´ enard, P., Domingues, O. D., Jonsson, A., Kaufmann, E.,
Leurent, E., and Valko, M. Fast active learning for pure
exploration in reinforcement learning. In Proceedings of
the 38th International Conference on Machine Learning
(ICML) , volume 139 of Proceedings of Machine Learn-
ing Research , pp. 7599–7608. PMLR, 2021.Towards Theoretical Understanding of Inverse Reinforceme nt Learning
Metelli, A. M., Pirotta, M., and Restelli, M. Compatible
reward inverse reinforcement learning. In Advances in
Neural Information Processing Systems 30 (NeurIPS) ,
pp. 2050–2059, 2017.
Metelli, A. M., Ramponi, G., Concetti, A., and Restelli,
M. Provably efﬁcient learning of transferable rewards.
InProceedings of the 38th International Conference on
Machine Learning (ICML) , volume 139 of Proceedings
of Machine Learning Research , pp. 7665–7676. PMLR,
2021.
Ng, A. Y . and Russell, S. Algorithms for inverse reinforce-
ment learning. In Proceedings of the Seventeenth Inter-
national Conference on Machine Learning (ICML) , pp.
663–670. Morgan Kaufmann, 2000.
Osa, T., Pajarinen, J., Neumann, G., Bagnell, J. A., Abbeel,
P., and Peters, J. An algorithmic perspective on imitation
learning. Found. Trends Robotics , 7(1-2):1–179, 2018.
Pirotta, M. and Restelli, M. Inverse reinforcement learn-
ing through policy gradient minimization. In Proceed-
ings of the Thirtieth Conference on Artiﬁcial Intelligence
(AAAI) , pp. 1993–1999. AAAI Press, 2016.
Puterman, M. L. Markov Decision Processes: Discrete
Stochastic Dynamic Programming . Wiley Series in Prob-
ability and Statistics. Wiley, 1994.
Ramponi, G., Likmeta, A., Metelli, A. M., Tirinzoni, A.,
and Restelli, M. Truly batch model-free inverse rein-
forcement learning about multiple intentions. In The
23rd International Conference on Artiﬁcial Intelligence
and Statistics (AISTATS) , volume 108 of Proceedings
of Machine Learning Research , pp. 2359–2369. PMLR,
2020.
Ratliff, N. D., Bagnell, J. A., and Zinkevich, M. Maximum
margin planning. In Proceedings of the Twenty-Third
International Conference on Machine Learning (ICML) ,
volume 148 of ACM International Conference Proceed-
ing Series , pp. 729–736. ACM, 2006.
Rockafellar, R. T. and Wets, R. J. Variational Analysis ,
volume 317 of Grundlehren der mathematischen Wis-
senschaften . Springer, 1998.
Sutton, R. S. and Barto, A. G. Reinforcement learning - an
introduction . Adaptive computation and machine learn-
ing. MIT Press, 1998.
Syed, U. and Schapire, R. E. A game-theoretic approach to
apprenticeship learning. pp. 1449–1456, 2007.
Vroman, M. C. Maximum likelihood inverse reinforcement
learning . Rutgers The State University of New Jersey-
New Brunswick, 2014.Weissman, T., Ordentlich, E., Seroussi, G., Verdu, S., and
Weinberger, M. J. Inequalities for the l1 deviation of the
empirical distribution. Hewlett-Packard Labs, Tech. Rep ,
2003.
Wu, X., Xiao, L., Sun, Y ., Zhang, J., Ma, T., and He, L. A
survey of human-in-the-loop for machine learning. Fu-
ture Gener. Comput. Syst. , 135:364–381, 2022.
Zeng, S., Li, C., Garcia, A., and Hong, M. Maximum-
likelihood inverse reinforcement learning with ﬁnite-
time guarantees. In Advances in Neural Information Pro-
cessing Systems (NeurIPS) , 2022.
Ziebart, B. D., Maas, A. L., Bagnell, J. A., and Dey, A. K.
Maximum entropy inverse reinforcement learning. In
Proceedings of the Twenty-Third Conference on Artiﬁ-
cial Intelligence (AAAI) , pp. 1433–1438. AAAI Press,
2008.Towards Theoretical Understanding of Inverse Reinforceme nt Learning
A. Additional Related Works
In this appendix, we report additional related works concer ning sample complexity analysis for speciﬁc IRL algorithms
and reward-free exploration.
Sample Complexity of IRL Algorithms Differently from forward RL, the theoretical understandin g of the IRL problem
is largely less established and the sample complexity analy sis proposed in the literature often limit to speciﬁc algori thms.
In the class of feature expectation approaches, the seminal work ( Abbeel & Ng ,2004 ) propose IRL algorithms guaranteed
to output an ǫ-optimal policy (made of a mixture of Markov policies) after rO´
k
ǫ2p1´γq2log`1
δ˘¯
trajectories (ideally of
inﬁnite length). The result holds in a discounted setting (b eingγthe discount factor) under the assumption that the true re-
ward function rpsq “wTφpsqis state-only and linear in some known featuresφof dimensionality k. In ( Syed & Schapire ,
2007 ), a game-theoretic approach to IRL, named MWAL , is proposed improving ( Abbeel & Ng ,2004 ) in terms of compu-
tational complexity and allowing the absence of an expert, p reserving similar theoretical guarantees in the same setti ng.
Modular IRL (Vroman ,2014 ), that integrates supervised learning capabilities in the IRL algorithm, is guaranteed to
produce an ǫ-optimal policy after rO´
SA
p1´γq2ǫ2log`1
δ˘¯
trajectories. This class of algorithms, however, requires , as an
inner step, to compute the optimal policy pπfor every candidate reward function pr. This step (and the corresponding sample
complexity) is somehow hidden in the analysis since they eit her assume the knowledge of the transition model and apply
dynamic programming (e.g., Vroman ,2014 ) or the access to a black-box RL algorithm (e.g., Abbeel & Ng ,2004 ). In the
class of maximum entropy approaches ( Ziebart et al. ,2008 ), theMaximum Likelihood IRL (Zeng et al. ,2022 ) con-
verges to a stationary solution with rOpǫ´2qtrajectories for non-linear reward parametrization (with bounded gradient and
Lipschitz smooth), when the underlying Markov chain is ergo dic. Furthermore, the authors prove that, when the reward is
linear in some features, the recovered solution correspond s toMaximum Entropy IRL (Ziebart et al. ,2008 ). Concern-
ing the gradient-based approaches, ( Pirotta & Restelli ,2016 ) and ( Ramponi et al. ,2020 ) prove ﬁnite-sample convergence
guarantee to the expert’s weight under linear parametrizat ion as a function of the accuracy of the gradient estimation. Sur-
prisingly, a theoretical analysis of the IRL progenitor alg orithm of ( Ng & Russell ,2000 ) has been proposed only recently
in (Komanduru & Honorio ,2019 ). Aβ-strict separability setting is enforced in which the rewar ds are assumed to lead
to a suboptimality gap of at least βą0when playing any non-optimal action. For ﬁnite MDPs, known e xpert’s pol-
icy, under the demanding assumption that each state is reach able in one step with a minimum probability αą0, and
focusing on state-only reward, the authors prove that the al gorithm outputs a β-strict separable feasible reward in at most
rO´
1`γ2Ξ2
αβ2p1´γq4log`1
δ˘¯
trajectories, where ΞďSis the number of possible successor states. Recently, an app roach with
theoretical guarantees has been proposed for continuous st ates ( Dexter et al. ,2021 ).
Reward-Free Exploration Reward-free exploration (RFE, Jin et al. ,2020 ;Kaufmann et al. ,2021 ;M´ enard et al. ,2021 )
is a setting for pure exploration in MDPs composed of two phas es: exploration and planning. In the exploration phase,
the agent learns an estimated transition model ppwithout any reward feedback. In the planning phase, the agen t is faced
with a reward function rand has to output an estimated optimal policy pπ˚, usingppsince no further interaction with
the environment is admitted. In this sense, RFE shares this t wo-phase procedure with our IRL problem, but, instead of
theplanning phase, we face the computation of the feasible reward set.12In RFE exploration, the sample complexity is
computed against the performance of the learned policy pπ˚under the reward r, i.e.,V˚p¨;rq ´Vpπ˚p¨;rq, whose lower
bound of the sample complexity has order Ω´
H2SA
ǫ2`
Hlog`1
δ˘
`S˘¯
(Jin et al. ,2020 ;Kaufmann et al. ,2021 ). The best
known algorithm, RF-Express , proposed in ( M´ enard et al. ,2021 ) archives an almost-matching sample complexity of
orderΩ´
H3SA
ǫ2`
log`1
δ˘
`S˘¯
. The relevant connection with what we present in this paper i s the fact that the derivation of
the lower bounds shares similarity especially in the constr uction of the instances. Nevertheless, in the time-inhomog eneous
case, we achieve a higher lower bound of order Ω´
H3SA
ǫ2`
log`1
δ˘
`S˘¯
. The connection between IRL and RFE should
be investigated in future works, as also mentioned in ( Lindner et al. ,2022 ).
B. Proofs
In this appendix, we report the proofs we omitted in the main p aper.
12As shown in previous works, the computation of the feasible r eward set can be formulated with a linear feasibility prob-
lem(Ng & Russell ,2000 ).Towards Theoretical Understanding of Inverse Reinforceme nt Learning
B.1. Proofs of Section 3
Lemma B.1. Letrbe feasible for the IRL problem pM,πEqbounded in r´1,1s(i.e.,prPR) and deﬁned according to
Lemma 3.1asrhps,aq “ ´Ahps,aq1tπE
hpa|sq“0u`Vhpsq ´phVh`1ps,aq. Let pxM,pπEqbe an IRL problem and deﬁne for
every ps,a,h q PSˆAˆ/llbracketH/rrbracket:
ǫhps,aq:“ ´Ahps,aq´
1tπE
hpa|sq“0u´1tpπE
hpa|sq“0u¯
` ppph´pphqVh`1q ps,aq.
Then, the reward function prdeﬁned according to Lemma 3.1asprhps,aq “ ´pAhps,aq1tpπE
hpa|sq“0u`pVhpsq´phpVh`1ps,aq
for every ps,a,h q PSˆAˆ/llbracketH/rrbracketwith:
pAhps,aq “Ahps,aq
1`ǫ,pVhpsq “Vhpsq
1`ǫ,pVH`1psq “0.
whereǫ:“max ps,a,h qPSˆAˆ/llbracketH/rrbracket|ǫhps,aq|, is feasible for the IRL problem pxM,pπEqand bounded in r´1,1s(i.e.,prPpR).
Proof. Given the reward function rhps,aq “ ´Ahps,aq1tπE
hpa|sq“0u`Vhpsq ´phVh`1ps,aq, we deﬁne the reward
function:
rrhps,aq “ ´Ahps,aq1tpπE
hpa|sq“0u`Vhpsq ´pphVh`1ps,aq,
that, thanks to Lemma 3.1, makes policy pπEoptimal. However, it is not guaranteed that rrPpRsince it can take values
larger than 1. Thus, we deﬁne the reward:
prhps,aq “rrhps,aq
1`ǫ“ ´Ahps,aq
1`ǫ1tpπE
hpa|sq“0u`Vh
1`ǫpsq ´pphVh`1
1`ǫps,aq,
which simply scales rrhand preserves the optimality of pπE. We now prove that prhps,aqis bounded in r´1,1s. To do so,
we prove that rrhps,aqis bounded in r´p1`ǫq,p1`ǫqs:
|rrhps,aq| ď |rhps,aq| ` |rrhps,aq ´rhps,aq|
“1`ˇˇˇ´Ahps,aq1tpπE
hpa|sq“0u`pphVh`1psq ´´
´Ahps,aq1tπE
hpa|sq“0u`phVh`1psq¯ˇˇˇ
“1` |ǫhps,aq| ď1`ǫ.
Theorem 3.2 (Lipschitz Continuity) .LetRandpRbe the feasible reward sets of the IRL problems pM,πEqandpxM,pπEq.
Then, it holds that:13
HdGpR,pRq ď2ρGppM,πEq,pxM,pπEqq
1`ρGppM,πEq,pxM,pπEqq, (5)
whereρGp¨,¨qis a (pre)metric between IRL problems, deﬁned as:
ρGppM,πEq,pxM,pπEqq:“max
ps,a,h qPSˆAˆ/llbracketH/rrbracketpH´h`1q
ˆ´ˇˇˇ1tπE
hpa|sq“0u´1tpπE
hpa|sq“0uˇˇˇ`}php¨|s,aq´pphp¨|s,aq}1¯
.
Proof. Letrras deﬁned in the proof of Lemma B.1. Then, we have:
|rhps,aq ´prhps,aq| “ˇˇˇˇrhps,aq ´rrhps,aq
1`ǫˇˇˇˇ
13This implies the standard Lipschitz continuity, by simply b ounding2ρGppM,πEq,pxM,pπEqq
1`ρGppM,πEq,pxM,pπEqqď2ρGppM,πEq,pxM,pπEqq.Towards Theoretical Understanding of Inverse Reinforceme nt Learning
ď1
1`ǫp|rhps,aq ´rrhps,aq| `ǫ|rhps,aq|q
ď2ǫ
1`ǫ.
By recalling that2ǫ
1`ǫis a non-decreasing function of ǫ, we bound it by replacing ǫwith an upper bound:
ǫ“max
ps,a,h qPSˆAˆ/llbracketH/rrbracket|ǫhps,aq|
ďmax
ps,a,h qPSˆAˆ/llbracketH/rrbracketpH´h`1q”ˇˇˇ1tπE
hpa|sq“0u´1tpπE
hpa|sq“0uˇˇˇ` }php¨|s,aq ´pphp¨|s,aq}1ı
“:ρGppM,πEq,pxM,pπEqq,
where we used H¨ older’s inequality recalling that |Vh`1ps,aq| ďH´hand |Ahps,aq| ďH´h`1. Clearly,
ρGppM,πEq,pxM,pπEqqis a (pre)metric.
Fact B.1. There exist two MDP \RMandxMwith transition models pandpprespectively, an expert’s policy πEand a
reward function rhps,aq “ ´Ahps,aq1tπEpa|sq“0u`Vhpsq ´phVh`1psqfeasible for the IRL problem pM,πEqbounded
inr´1,1s(i.e.,rPR) such that the reward function prhps,aq “ ´Ahps,aq1tπEpa|sq“0u`Vhpsq ´pphVh`1psqis feasible
for the IRL problem pxM,πEqnot bounded in r´1,1s.
Proof. We consider the MDP \R in Figure 3with optimal policy and reward function deﬁned for every hP/llbracketH/rrbracketand
H“10as:
πE
hps1q “a1, πE
hps2q “a2,
rhps1,a1q “rhps2,a1q “0, rhps1,a2q “ ´1, rhps2,a2q “1.
Simple calculations lead to the V-function and advantage fu nction values:
VπE
hps1q “0, VπE
hps2q “H´h`1,
AπE
hps1,a1q “0, AπE
hps1,a2q “ ´1` pH´hq{10, AπE
hps2,a1q “ ´1´ pH´hq{10, AπE
hps2,a2q “0.
We consider as alternative transition model pp“1´p. After tedious calculations we obtain the alternative rewa rd function:
prhps1,a1q “ ´pH´hq,prhps1,a2q “ ´1`8pH´hq{10,prhps2,a1q “8pH´hq{10,prhps2,a2q “H´h.
It is simple to observe that for some ps,a,h qwe have |prhps,aq| ą1.
s1 s2 a1 a2
a2a1
9{101{10
1{109{10
Figure 3. The MDP \R employed in Fact B.1.
B.2. Proofs of Section 4
Theorem 4.1 (Relationships between d-IRL problems) .Let us introduce the graphical convention for cą0:Towards Theoretical Understanding of Inverse Reinforceme nt Learning
x-IRL y-IRLc
meaning that any pǫ,δq-PACx-IRL algorithm is pcǫ,δq-PACy-IRL. Then, the following statements hold:
dG-IRL dG
Q˚-IRL dG
V˚-IRL .2H
H 2H
Proof. LetAbe an pǫ,δq-PACdG-IRL algorithm. This means that with probability at least 1´δ, we have that for any IRL
problemHdGpR,pRτq ďǫ. We introduce the following visitation distributions, deﬁ ned for every s,s1PS,h,lP/llbracketH/rrbracketwith
lěh, anda,a1PA:
ηπ
s,a,h,l ps1,a1q “P
M,π`
sl“s1,al“a1|sh“s,ah“a˘
, ηπ
s,h,lps1,a1q “ÿ
aPAπhpa|sqηπ
s,a,h,l ps1,a1q.
dG-IRL ÑdG
Q˚-IRL Let us consider the optimal Q-function difference and let π˚an optimal policy under the reward
functionr, we have:
Q˚
hps,a;rq ´Q˚
hps,a;prq ďQπ˚
hps,a;rq ´Qπ˚
hps,a;prq
“Hÿ
l“hÿ
ps1,a1qPSˆAηπ˚
s,a,h,l ps1,a1qprlps1,a1q ´prlps1,a1qq
ďmax
ps,a,h qPSˆAˆ/llbracketH/rrbracket|rhps,aq ´prhps,aq|Hÿ
l“hÿ
ps1,a1qPSˆAηπ˚
s,a,h,l ps1,a1q
loooooooooooooomoooooooooooooon
“1
“ pH´h`1qmax
ps,a,h qPSˆAˆ/llbracketH/rrbracket|rhps,aq ´prhps,aq|
ďHmax
ps,a,h qPSˆAˆ/llbracketH/rrbracket|rhps,aq ´prhps,aq|.
As a consequence, we have:
HdG
Q˚pR,pRτq ďHHdGpR,pRτq.
dG-IRL ÑdG
V˚-IRL Let us consider the value functions and let π˚(resp.pπ˚) be an optimal policy under reward function
r(resp.pr), we have:
V˚
hps;rq ´Vpπ˚
hps;rq “Vπ˚
hps;rq ´Vpπ˚
hps;rq ˘Vpπ˚
hps;prq
ďVπ˚
hps;rq ´Vπ˚
hps;prq `Vpπ˚
hps;prq ´Vpπ˚
hps;rq
“Hÿ
l“hÿ
ps1,a1qPSˆAηπ˚
s,h,lps1,a1qprlps1,a1q ´prlps1,a1qq `Hÿ
l“hÿ
ps1,a1qPSˆAηpπ˚
s,h,lps1,a1qprlps1,a1q ´prlps1,a1qq
ďmax
ps,a,h qPSˆAˆ/llbracketH/rrbracket|rhps,aq ´prhps,aq|¨
˝Hÿ
l“hÿ
ps1,a1qPSˆAηπ˚
s,h,lps1,a1q `Hÿ
l“hÿ
ps1,a1qPSˆAηpπ˚
s,h,lps1,a1q˛
‚
“2pH´h`1qmax
ps,a,h qPSˆAˆ/llbracketH/rrbracket|rhps,aq ´prhps,aq|
ď2Hmax
ps,a,h qPSˆAˆ/llbracketH/rrbracket|rhps,aq ´prhps,aq|.Towards Theoretical Understanding of Inverse Reinforceme nt Learning
Thus, it follows that:
HdG
V˚pR,pRτq ď2HHdGpR,pRτq.
dG
Q˚-IRL ÑdG
V˚-IRL To prove this result, we need to introduce further tools. Spe ciﬁcally, we introduce the Bellman
expectation operator and the Bellman optimal operator, deﬁ ned for a reward function r, policyπ,ps,hq PSˆ/llbracketH/rrbracketand
functionf:SÑR:
T˚
r,hfpsq “max
aPAtrhps,aq `phfps,aqu, Tπ
r,hfpsq “πhprhps,aq `phfps,aqq.
We recall the ﬁxed-point properties: T˚
r,hV˚
h“V˚
handTπ
r,hVπ
h“Vπ
h. Letπ˚(resp.pπ˚) be an optimal policy under
rewardr(resp.pr). Let us consider the following derivation:
V˚
hps;rq ´Vpπ˚
hps;rq “T˚
r,hV˚
hps;rq ´Tpπ˚
r,hVpπ˚
hps;rq ˘Tπ˚
r,hV˚
hps;prq ˘Tπ˚
pr,hV˚
hps;prq ˘T˚
pr,hV˚
hps;prq ˘Tpπ˚
r,hVpπ˚
hps;prq
“Tπ˚
r,hV˚
hps;rq ´Tπ˚
r,hV˚
hps;prq `Tπ˚
r,hV˚
hps,prq ´Tπ˚
pr,hV˚
hps;prq `Tπ˚
pr,hV˚
hps;prq ´T˚
pr,hV˚
hps;prqlooooooooooooooooomooooooooooooooooon
ď0
`Tpπ˚
pr,hV˚
hps;prq ´Tpπ˚
r,hV˚
hps;prq `Tpπ˚
r,hV˚
hps;prq ´Tpπ˚
r,hV˚
hps;rq
“π˚
hphpV˚
h`1p¨;rq ´V˚
h`1p¨;prqqpsq `π˚
hprh´prhqpsq
`pπ˚
hpprh´rhqpsq `pπ˚phpV˚
h`1p¨;prq ´Vpπ˚
h`1p¨;rqqpsq
“ pπ˚
h´pπ˚
hqpQ˚
hp¨;rq ´Q˚
hp¨;prqqpsq `pπ˚phpV˚
h`1p¨;rq ´Vpπ˚
h`1p¨;rqqpsq.
Let us apply the L8-norm over the state space and the triangular inequality, we have:
›››V˚
hp¨;rq ´Vpπ˚
hp¨;rq›››
8ď }pπ˚
h´pπ˚
hqpQ˚
hp¨;rq ´Q˚
hp¨;prqqp¨q}8`›››pπ˚phpV˚
h`1p¨;rq ´Vpπ˚
h`1p¨;rqqp¨q›››
8
ď2}Q˚
hp¨;rq ´Q˚
hp¨;prqqp¨q}8`›››V˚
h`1p¨;rq ´Vpπ˚
h`1p¨;rq›››
8.
By unfolding the recursion over h, we obtain:
›››V˚
hp¨;rq ´Vpπ˚
hp¨;rq›››
8ď2Hÿ
l“h`1}Q˚
lp¨;rq ´Q˚
lp¨;prqqp¨q}8.
Thus, we have:
max
ps,hqPSˆ/llbracketH/rrbracketˇˇˇV˚
hps;rq ´Vpπ˚
hps;rqˇˇˇď2Hmax
ps,a,h qPSˆAˆ/llbracketH/rrbracket|Q˚
hps,a;rq ´Q˚
hps,a;prq|.
Since the derivation is carried out for arbitrary pπ˚, it follows that:
HdG
V˚pR,pRτq ď2HHdG
Q˚pR,pRτq.
B.3. Proofs of Section 5
Theorem 5.1 (Lower Bound for dG-IRL) .LetA“ pµ,τqbe an pǫ,δq-PAC algorithm for dG-IRL. Then, there exists an IRL
problem pM,πEqsuch that, if δď1{32,Sě9,Aě2, andHě12, the expected sample complexity is lower bounded
by:
•if the transition model pis time-inhomogeneous:
E
pM,πEq,Arτs ě1
1024H3SA
ǫ2ˆ1
2logˆ1
δ˙
`1
5S˙
;Towards Theoretical Understanding of Inverse Reinforceme nt Learning
•if the transition model pis time-homogeneous:
E
pM,πEq,Arτs ě1
512H2SA
ǫ2ˆ1
2logˆ1
δ˙
`1
5S˙
,
whereEpM,πEq,Adenotes the expectation w.r.t. the probability measure PpM,πEq,A.
Proof. We put together the results of Theorem B.2and Theorem B.3, by recalling that maxta,bu ěa`b
2, or, equivalently,
assuming to observe instances like the ones of Theorem B.2w.p.1{2as well as those of Theorem B.3.
Theorem B.2. LetA“ pµ,τqbe an pǫ,δq-PAC algorithm for dG-IRL. Then, there exists an IRL problem pM,πEqsuch
that, ifǫď1,δă1{16,Sě9,Aě2, andHě12, the expected sample complexity is lower bounded by:
•if the transition model pis time-inhomogeneous:
E
pM,πEq,Arτs ě1
2048H3SA
ǫ2logˆ1
δ˙
;
•if the transition model pis time-homogeneous:
E
pM,πEq,Arτs ě1
1024H2SA
ǫ2logˆ1
δ˙
.
Proof. Step 1: Instances Construction The construction of the hard MDP \R instances follows similar steps as the ones
presented in the constructions of lower bounds for policy le arning ( Domingues et al. ,2021 ) and the hard instances are
reported in Figure 4in a semi-formal way. The state space is given by S“ tsstart,sroot,s´,s`,s1,...,sSuand the action
space is given by A“ ta0,a1,...,aAu. The transition model is described below and the horizon is Hě3. We introduce
the constant HP/llbracketH/rrbracket, whose value will be chosen later. Let us observe, for now, th at ifH“1, the transition model is
time-homogeneous.
The agent begins in state sstart, where every action has the same effect. Speciﬁcally, if the stagehăH, then there is
probability 1{2to remain in sstartand a probability 1{2to transition to sroot. Instead, if hěH, the state transitions to
srootdeterministically. From state sroot, every action has the same effect and the state transitions w ith equal probability 1{S
to a state siwithiP/llbracketS/rrbracket. In all states si, apart from a speciﬁc one, i.e., state s˚, all actions have the same effect, i.e.,
transitioning to states s´ands`with equal probability 1{2. States˚behaves as the other ones if the stage h‰h˚, where
h˚P/llbracketH/rrbracketis a predeﬁned stage. If, instead, h“h˚, all actions aj‰a˚behave like in the other states, while for action a˚,
we have a 1{2`ǫ1probability of reaching s`(and consequently probability 1{2´ǫ1of reaching s´), withǫ1P r0,1{4s.
Notice that, having ﬁxed H, the possible values of h˚aret3,...,2`Hu. Statess`ands´are absorbing states. The
expert’s policy always plays action a0.
Let us consider the base instance M0in which there is no state behaving like s˚. Additionally, by varying the triple
ℓ:“ ps˚,a˚,h˚q P ts1,...,sSu ˆ ta1,...,aAu ˆ/llbracket3,H`2/rrbracket“:I, we can construct the class of instances denoted by
M“ tMℓ:ℓP t0u YIu.
Step 2: Feasible Set Computation Let us consider an instance MℓPM, we now seek to provide a lower bound to the
Hausdorff distance HdGpRM0,RMℓq. To this end, we focus on the triple ℓ“ ps˚,a˚,h˚qand we enforce the convenience
of action a0over action a˚. For the base MDP \RM0, letr0PRM0, we have:
r0
h˚ps˚,a0q `1
2Hÿ
l“h˚`1`
r0
lps´q `r0
lps`q˘
ěr0
h˚ps˚,a˚q `1
2Hÿ
l“h`1`
r0
lps´q `r0
lps`q˘
ù ñr0
h˚ps˚,a0q ěr0
h˚ps˚,a˚q,
For the alternative MDP \RMℓ, letrℓPRMℓ, we have:
rℓ
h˚ps˚,a0q `1
2Hÿ
l“h˚`1`
rℓ
lps´q `rℓ
lps`q˘
ěrℓ
h˚ps˚,a˚q `Hÿ
l“h˚`1ˆˆ1
2´ǫ1˙
rℓ
lps´q `ˆ1
2`ǫ1˙
rℓ
lps`q˙Towards Theoretical Understanding of Inverse Reinforceme nt Learning
sstart
sroot
... ...s˚s1 sS
s` s´hăHw.p.1
2
w.p.1
2orhěH
w.p.1
Sw.p.1
Sw.p.1
S
w.p.1
2w.p.1
2w.p.1
2
w.p.1
2h“h˚w.p.1
2`ǫ1h“h˚w.p.1
2´ǫ1
w.p.1
2w.p.1
2
Figure 4. Semi-formal representation of the the hard instances MDP \R used in the proof of Theorem B.2.
ù ñrℓ
h˚ps˚,a0q ěrℓ
h˚ps˚,a˚q ´ǫ1Hÿ
l“h˚`1`
rℓ
lps´q ´rℓ
lps`q˘
.
In order to lower bound the Hausdorff distance HdGpRM0,RMℓq, we set for Mℓ:
rℓ
lps´q “ ´rℓ
lps`q “1, rℓ
h˚ps˚,a˚q “1, rℓ
h˚ps˚,a0q “1´2ǫ1pH´h˚q.
Then, for notational convenience, for the MDP \RM0, we setx:“r0
h˚ps˚,a0qandy:“r0
h˚ps˚,a˚q:
HdGpRM0,RMℓq ěmin
x,yPr´1,1s
yěxmax/visualspace
|x´1|,ˇˇy´1`2ǫ1pH´h˚qˇˇ(
“ǫ1pH´h˚q.
We enforce the following constraint on this quantity:
@h˚P/llbracket3,H`2/rrbracket:pH´h˚qǫ1ě2ǫù ñǫ1ěmax
h˚P/llbracket3,H`2/rrbracket2ǫ
pH´h˚q“2ǫ
pH´H´2q. (9)
Notice that ǫ1ď1{4whenever HěH`10.
Step 3: Lower bounding Probability Let us consider an pǫ,δq-correct algorithm Athat outputs the estimated feasibleTowards Theoretical Understanding of Inverse Reinforceme nt Learning
setpR. Thus, for every ıPI, we can lower bound the error probability:
δě sup
allMMDP\R and expert policies πP
pM,πq,A´
HdG´
RM,pR¯
ěǫ¯
ěsup
MPMP
pM,πq,A´
HdG´
RM,pR¯
ěǫ¯
ěmax
ℓPt0,ıuP
pMℓ,πq,A´
HdG´
RMℓ,pR¯
ěǫ¯
.
For every ıPI, let us deﬁne the identiﬁcation function :
Ψı:“argmin
ℓPt0,ıuHdG´
RMℓ,pR¯
.
LetP t0,ıu. IfΨı“, then,HdGpRMΨı,RMq “0. Otherwise, if Ψı‰, we have:
HdG`
RMΨı,RM˘
ďHdG´
RMΨı,pR¯
`HdG´
pR,RM¯
ď2HdG´
pR,RM¯
,
where the ﬁrst inequality follows from triangular inequali ty and the second one from the deﬁnition of identiﬁcation fun ction
Ψı. From Equation ( 9), we have that HdG`
RMΨı,RM˘
ě2ǫ. Thus, it follows that HdG´
pR,RM¯
ěǫ. This implies
the following inclusion of events for P t0,ıu:
!
HdG´
pR,RM¯
ěǫ)
Ě tΨı‰u.
Thus, we can proceed by lower bounding the probability:
max
ℓPt0,ıuP
pMℓ,πq,A´
HdG´
RMℓ,pR¯
ěǫ¯
ěmax
ℓPt0,ıuP
pMℓ,πq,ApΨı‰ℓq
ě1
2„
P
pM0,πq,ApΨı‰0q `P
pMı,πq,ApΨı‰ıq
“1
2„
P
pM0,πq,ApΨı‰0q `P
pMı,πq,ApΨı“0q
,
where the second inequality follows from the observation th atmaxta,bu ě1
2pa`bqand the equality from observing
thatΨıP t0,ıu. The intuition behind this derivation is that we lower bound the probability of making a mistake ě
ǫwith the probability of failing in identifying the true unde rlying problem. We can now apply the Bretagnolle-Huber
inequality ( Lattimore & Szepesv´ ari ,2020 , Theorem 14.2) (also reported in Theorem E.1for completeness) with P“
PpM0,πq,A,Q“PpM0,πqA, andA“ tΨı‰0u:
P
pM0,πq,ApΨı‰0q `P
pMı,πq,ApΨı“0q ě1
2expˆ
´DKLˆ
P
pM0,πq,A,P
pMı,πq,A˙˙
.
Step 4: KL-divergence Computation LetMPM, we denote with PA,M,πthe joint probability distribution of all events
realized by the execution of the algorithm in the MDP \R (the presence of πis irrelevant as we assume it known):
P
pM,πq,A“τź
t“1ρtpst,at,ht|Ht´1qphtps1
t|st,atq.
whereHt´1“ ps1,a1,h1,s1
1,...,s t´1,at´1,ht´1,s1
t´1qis the history. Let ıPIand denote with p0andpıthe transition
models associated with M0andMı. Let us now move to the KL-divergence:
DKL`
PpM0,πq,A,PpMı,πq,A˘
“E
pM0,πq,A«τÿ
t“1DKL`
p0
htp¨|st,atq,pı
htp¨|st,atq˘ﬀ
ďE
pM0,πq,A”
Nτ
h˚ps˚,a˚qı
DKL´
p0
h˚p¨|s˚,a˚q,pı
h˚p¨|s˚,a˚q¯Towards Theoretical Understanding of Inverse Reinforceme nt Learning
ď8pǫ1q2E
pM0,πq,A”
Nτ
h˚ps˚,a˚qı
.
having observed that the transition models differ in ı“ ps˚,a˚,h˚qand deﬁned Nτ
h˚ps˚,a˚q “řτ
t“11tpst,at,htq “
ps˚,a˚,h˚quand the last passage is obtained by Lemma E.4withD“2(andǫ“2ǫ1). Putting all together, we have:
δě1
4expˆ
´8E
pM0,πq,A”
Nτ
h˚ps˚,a˚qı
pǫ1q2˙
ù ñE
pM0,πq,A”
Nτ
h˚ps˚,a˚qı
ělog1
4δ
8pǫ1q2“pH´H´2q2log1
4δ
32ǫ2.
Thus, summing over ps˚,a˚,h˚q PI, we have:
E
pM0,πq,Arτs ěÿ
ps˚,a˚,h˚qPIE
pM0,πq,A”
Nτ
h˚ps˚,a˚qı
“ÿ
ps˚,a˚,h˚qPIpH´H´2q2log1
4δ
32ǫ2
“SAHpH´H´2q2
32ǫ2log1
4δ.
The number of states is given by S“ |S| “S`4, the number of actions is given by A“ |A| “A`1. Let us ﬁrst
consider the time-homogeneous case, i.e., H“1:
E
pM0,πq,Arτs ěpS´4qpA´1qpH´3q2
32ǫ2log1
4δ.
Forδă1{16,Sě9,Aě2,Hě10, we obtain:
E
pM0,πq,Arτs ěSAH2
1024ǫ2log1
δ.
For the time-inhomogeneous case, instead, we select H“H{2, to get:
E
pM0,πq,Arτs ěpS´4qpA´1qpH{2qpH´H{2´2q2
ǫ2log1
4δ.
Forδă1{16,Sě9,Aě2,Hě12, we obtain:
E
pM0,πq,Arτs ěSAH3
2048ǫ2log1
δ.Towards Theoretical Understanding of Inverse Reinforceme nt Learning
Theorem B.3. LetA“ pµ,τqbe an pǫ,δq-PAC algorithm for dG-IRL. Then, there exists an IRL problem pM,πEqsuch
that, ifǫď1,δď1{2,Sě16,Aě2,Hě131, the expected sample complexity is lower bounded by:
•if the transition model pis time-inhomogeneous:
E
pM,πEq,Arτs ě1
5120S2AH3
ǫ2;
•if the transition model pis time-homogeneous:
E
pM,πEq,Arτs ě1
2560S2AH2
ǫ2.
Proof. Step 1: Instances Construction The construction of the hard MDP \R instances for this second bound follows
steps similar to those of reward free exploration ( Jin et al. ,2020 ) and the instances are reported in Figure 5in a semi-
formal way. The state space is given by S“ tsstart,sroot,s1,...,sS,s1
1,...,s1
Suand the action space is given by A“
ta0,a1,...,aAu. We assume Sto be divisible by 16. The transition model is described below and the horizon is Hě3.
The agent begins in state sstart, where every action has the same effect. Speciﬁcally, if the stagehăH(HP/llbracketH/rrbracket,
whose value will be chosen later), then there is probability 1{2to remain in sstartand a probability 1{2to transition to
sroot. Instead, if hěH, the state transitions to srootdeterministically. From state sroot, every action has the same effect
and the state transitions with equal probability 1{Sto a state siwithiP/llbracketS/rrbracket. In every state siand every stage h, action
a0allows reaching states s1
1,...,s1
Swith equal probability 1{S. Instead, by playing the other actions ajwithjě1at
stageh, the probability distribution of the next state is given by phps1
k|si,ajq “ p1`ǫ1vpsi,aj,hq
kq{Swhere the vector
vpsi,aj,hq“ pvpsi,aj,hq
1,...,vpsi,aj,hq
Sq PV, whereV:“ tt´1,1uS:řS
j“1vj“0uandǫ1P r0,1{2s. Notice that, having
ﬁxedH, the possible values of haret3,...,2`Hu. Statess1
1,...,s1
Sare absorbing states. The expert’s policy always
plays action a0.
Let us introduce the set I:“ ts1,...,sSu ˆ ta1,...,aAu ˆ/llbracket3,H`2/rrbracket. Letv“ pvıqıPIPVIwhich is the set of vectors
having as components the elements vıdetermining the probability distribution of the next state starting from the triple ıPI.
We denote with Mvthe MDP\R induced by v. We can construct the class of instances denote by M“ tMv:vPVIu.
Moreover, we denoted with MvıÐwthe instance in which we replace the ıcomponent of v, i.e.,vı, withwPVandMvıÐ0
the instance in which we replace the ıcomponent of v, i.e.,vı, with the zero vector.
Step 2: Feasible Set Computation Thanks to Lemma E.6, we know that there exists a subset VĂVof cardinality at
least |V| ě2S{5such that for every v,w PVwithv‰wwe haveřS
j“1|vj´wj| ěS{16. Thus, we consider the set
VIĂVIand to build the instances vPVIandv,w PVwithv‰w. LetıPI, the induced instances are denoted by
MvıÐv,MvıÐwPM.
To lower bound the Hausdorff distance, we focus on the triple ı“ ps˚,a˚,h˚qand we enforce the convenience of action
a0over action a˚. For both MDP \RMvıÐvandMvıÐw, letrvPRMvıÐvandrwPRMvıÐw, we have:
rv
h˚ps˚,a0q `1
SHÿ
l“h˚`1Sÿ
j“1rv
lps1
jq ěrv
h˚ps˚,a˚q `Hÿ
l“h˚`1Sÿ
j“11`ǫ1vj
Srv
lps1
jq
ù ñrv
h˚ps˚,a0q ěrv
h˚ps˚,a˚q `ǫ1
SSÿ
j“1vjHÿ
l“h˚`1rv
lps1
jq.
rw
h˚ps˚,a0q `1
SHÿ
l“h˚`1Sÿ
j“1rw
lps1
jq ěrw
h˚ps˚,a˚q `Hÿ
l“h˚`1Sÿ
j“11`ǫ1w1
j
Srw
lps1
jq
ù ñrw
h˚ps˚,a0q ěrw
h˚ps˚,a˚q `ǫ1
SSÿ
j“1wjHÿ
l“h˚`1rw
lps1
jq. (10)Towards Theoretical Understanding of Inverse Reinforceme nt Learning
sstart
sroot
... ... s1 sS
s1
1 s1
2s1
S...hăHw.p.1
2
w.p.1
2orhěH
w.p.1
Sw.p.1
S
ajw.p.1`ǫ1vpsS,aj,hq
1
S
ajw.p.1`ǫ1vpsS,aj,hq
2
Sajw.p.1`ǫ1vpsS,aj,hq
S
Sajw.p.1`ǫ1vps1,aj,hq
1
Sajw.p.1`ǫ1vps1,aj,hq
2
S
ajw.p.1`ǫ1vps1,aj,hq
S
S
Figure 5. Semi-formal representation of the the hard instances MDP \R used in the proof of Theorem B.3.
In order to lower bound the Hausdorff distance HdG`
MvıÐv,MvıÐw˘
, we set for MvıÐv:
rv
lps1
jq “ ´vj, rv
h˚ps˚,a˚q “1, rv
h˚ps˚,a0q “1´ǫ1pH´h˚q.
We now want to ﬁnd the closest reward function rwfor the instance MvıÐw, recalling that there are at least S{16com-
ponents of the vectors vandwthat are different. Clearly, we can set rw
lps1
jq “rv
lps1
jq “ ´vjfor alljP/llbracketS/rrbracketin which
vj“wjsince this will not increase the Hausdorff distance and make the constraint in Equation ( 10) less restrictive. For
symmetry reasons, we can limit our reasoning to the case in wh ichvj“ ´1andwj“1for thejterms in which they are
different. This, way, the constraint becomes:
rw
h˚ps˚,a0qlooooomooooon
“:xěrw
h˚ps˚,a˚qlooooomooooon
“:y´Nv,w
Sǫ1pH´h˚q
`ˆ
1´Nv,w
S˙
ǫ1pH´h˚q1
pH´h˚q´
1´Nv,w
S¯Sÿ
j:vj‰wjHÿ
l“h˚`1rw
lps1
jq
loooooooooooooooooooooooooooomooooooooooooooooooooo ooooooon
“:z,
whereNv,w“řS
j“11tvj“wju. Notice that zP r´1,1s. Letα“Nv,w
S, the Hausdorff distance can be lower boundedTowards Theoretical Understanding of Inverse Reinforceme nt Learning
by:
HdG`
MvıÐv,MvıÐw˘
“ min
x,y,z Pr´1,1s
yěx´αǫ1pH´h˚q`p1´αqǫ1pH´h˚qzmax/visualspace
|x´1|,|y´ p1´ǫ1pH´h˚qq|,|z`1|(
ěmin
x,yPr´1,1s
yěx´αǫ1pH´h˚qmax/visualspace
|x´1|,|y´ p1´ǫ1pH´h˚qq|(
“1
2p1´αqǫ1pH´h˚q ěǫ1
32pH´h˚q,
where the ﬁrst inequality derives from the fact that to have a Hausdorff distance smaller than 1, we must take ză0at least
and the second inequality is obtained by recalling that 1´αě1
16for the packing argument.
We enforce the following constraint on this quantity:
@h˚P/llbracket3,H`2/rrbracket:ǫ1
32pH´h˚q ě2ǫù ñǫ1ěmax
h˚P/llbracket3,H`2/rrbracketǫ
64pH´h˚q“64ǫ
pH´H´2q. (11)
Notice that ǫ1ď1{2whenever HěH`130.
Step 3: Lower bounding Probability Let us consider an pǫ,δq-correct algorithm Athat outputs the estimated feasible set
pR. Thus, consider ıPIandvPVI, we can lower bound the error probability:
δě sup
allMMDP\R and expert policies πP
pM,πq,A´
HdG´
RM,pR¯
ěǫ¯
ěsup
MPMP
pM,πq,A´
HdG´
RM,pR¯
ěǫ¯
ěmax
wPVP
pMvıÐw,πq,A´
HdG´
RMvıÐw,pR¯
ěǫ¯
.
For every ıPIandvPVI, let us deﬁne the identiﬁcation function :
Ψı,v:“argmin
wPVHdG´
RMvıÐw,pR¯
.
LetwPV. IfΨı,v“w, then,HdGpRMvıÐΨı,v,RMvıÐwq “0. Otherwise, if Ψı,v‰w, we have:
HdGpRMvıÐΨı,v,RMvıÐwq ďHdGpRMvıÐΨı,v,pRq `HdGppR,RMvıÐwq ď2HdGppR,RMvıÐwq,
where the ﬁrst inequality follows from triangular inequali ty and the second one from the deﬁnition of identiﬁcation fun ction
Ψı,v. From Equation ( 11), we have that HdG´
RMΨıı,RMvı¯
ě2ǫ. Thus, it follows that HdGppR,RMvıÐwq ěǫ. This
implies the following inclusion of events for wPV:
!
HdGppR,RMvıÐwq ěǫ)
Ě tΨı,v‰wu.
Thus, we can proceed by lower bounding the probability:
max
wPVP
pMvıÐw,πq,A´
HdG´
RMvıÐw,pR¯
ěǫ¯
ěmax
wPVP
pMvıÐw,πq,ApΨı,v‰wq
ě1
|V|ÿ
wPVP
pMvıÐw,πq,ApΨı,v‰wq,
where the second inequality follows from bounding the maxim um of probability with the average. We can now apply the
Fano’s inequality (Theorem E.2) with reference probability P0“PpMvıÐ0,πq,A,Pw“PpMvıÐw,πq,A, andAw“ tΨı,v‰Towards Theoretical Understanding of Inverse Reinforceme nt Learning
wu:
1
|V|ÿ
wPVP
pMvıÐw,πq,ApΨı,v‰wq ě1´1
log|V|¨
˝1
|V|ÿ
wPVDKL˜
P
pMvıÐw,πq,A,P
pMvıÐ0,πq,A¸
´log2˛
‚. (12)
Step 4: KL-divergence Computation LetMbe an instance, we denote with PA,M,πthe joint probability distribution of
all events realized by the execution of the algorithm in the M DP\R (the presence of πis irrelevant as we assume it known):
P
pM,πq,A“τź
t“1ρtpst,at,ht|Ht´1qphtps1
t|st,atq.
whereHt´1“ ps1,a1,h1,s1
1,...,s t´1,at´1,ht´1,s1
t´1qis the history up to time t´1. LetıPIandvPVand denote
withpvıÐ0andpvıÐwthe transition models associated with MvıÐ0andMvıÐw. Let us now move to the KL-divergence
and denoting ı“ ps˚,a˚,h˚q: Thus, we have:
DKL˜
P
pMvıÐw,πq,A,P
pMvıÐ0,πq,A¸
“E
pMvıÐw,πq,A«τÿ
t“1DKL´
pvıÐw
htp¨|st,atq,pvıÐ0
htp¨|st,atq¯ﬀ
ďE
pMvıÐw,πq,A”
Nτ
h˚ps˚,a˚qı
DKL´
pvıÐw
h˚p¨|s˚,a˚q,pvıÐ0
h˚p¨|s˚,a˚q¯
ď2pǫ1q2E
pMvıÐw,πq,A”
Nτ
h˚ps˚,a˚qı
,
having observed that the transition models differ in ı“ ps˚,a˚,h˚qand deﬁned Nτ
h˚ps˚,a˚q “řτ
t“11tpst,at,htq “
ps˚,a˚,h˚quand the last passage is obtained by Lemma E.4withD“S. Plugging into Equation ( 12), we obtain:
δě1
|V|ÿ
wPVP
pMvıÐw,πq,ApΨı,v‰wq ù ñ1
|V|ÿ
wPVE
pMvıÐw,πq,A”
Nτ
h˚ps˚,a˚qı
ěp1´δqlog|V| ´log2
2pǫ1q2.
Since the derivation is carried out for every ıPIandvPVI, we can perform the summation over ıand the average over
v:
ÿ
ıPI1
|V||I|ÿ
vPVI1
|V|ÿ
wPVE
pMvıÐw,πq,A”
Nτ
h˚ps˚,a˚qı
“1
|V||I|ÿ
vPVIÿ
ıPIE
pMv,πq,A”
Nτ
h˚ps˚,a˚qı
ěSAHp1´δqlog|V| ´log2
2pǫ1q2.
Notice that we get a guarantee on a mean under the uniform dist ribution of the instances of the sample complexity. Thus,
there must exist one vhardPVsuch that:
E
pMvhard,πq,Arτs ěÿ
ıPIE
pMvhard,πq,A”
Nτ
h˚ps˚,a˚qı
ěSAHp1´δqlog|V| ´log2
2pǫ1q2.
Then, we select δď1{2, recall that |V| ě2S{5, we get:
E
pMvhard,πq,Arτs ěSAHS{10´log2
2pǫ1q2“SAHpH´H´2q2pS{10´log2 q
8912ǫ2
The number of states is given by S“ |S| “2S`2, the number of actions is given by A“ |A| “A`1. Let us ﬁrst
consider the time-homogeneous case, i.e., H“1, forSě16,Aě2,Hě130, we have:
E
pMvhard,πq,Arτs ěS2AH2
2560ǫ2.Towards Theoretical Understanding of Inverse Reinforceme nt Learning
For the time inhomogeneous case, we select H“H{2, to get, under the same conditions:
E
pMvhard,πq,Arτs ěS2AH3
5120ǫ2.Towards Theoretical Understanding of Inverse Reinforceme nt Learning
B.4. Proofs of Section 6
Theorem D.2 (Sample Complexity of US-IRL ).Letǫą0andδP p0,1q,US-IRL ispǫ,δq-PAC for dG-IRL and with
probability at least 1´δit stops after τsamples with:
•if the transition model pis time-inhomogeneous:
τď8H3SA
ǫ2ˆ
logˆSAH
δ˙
` pS´1qC˙
,
whereC“logpe{pS´1q ` p8eH2q{ppS´1qǫ2qplogpSAH {δq `4eqq;
•if the transition model pis time-homogeneous and :
τď8H2SA
ǫ2ˆ
logˆSA
δ˙
` pS´1qC2˙
,
whererC“logpe{pS´1q ` p8eH2q{ppS´1qǫ2qplogpSA{δq `4eqq.
Proof. We start with the case in which the transition model is time-i nhomogeneous. In this case, we introduce the following
good event:
E:“#
@tPN,@ps,a,h q PSˆAˆ/llbracketH/rrbracket:DKL´
ppt
hp¨|s,aq,php¨|s,aq¯
ďβ`
nt
hps,aq,δ˘
nt
hps,aq+
,
wherephis the true transition model and ppt
his its estimate via Equation ( 3) at timet. Thanks to Lemma B.4, we have that
PpM,πEq,ApEq ě1´δ. Thus, under the good event E, we apply Theorem 3.2:
HdGpR,pRτq ď2ρGppM,πEq,pxMt,pπE,tqq
1`ρGppM,πEq,pxMt,pπE,tqq
ď2ρGppM,πEq,pxM,pπEqq
ď2 max
ps,a,h qPSˆAˆ/llbracketH/rrbracketpH´h`1q´ˇˇˇ1tπE
hpa|sq“0u´1tpπE
hpa|sq“0uˇˇˇ` }php¨|s,aq ´pphp¨|s,aq}1¯
ď2 max
ps,a,h qPSˆAˆ/llbracketH/rrbracketpH´h`1q }php¨|s,aq ´pphp¨|s,aq}1
ď2?
2 max
ps,a,h qPSˆAˆ/llbracketH/rrbracketpH´h`1qc
DKL´
ppt
hp¨|s,aq,php¨|s,aq¯
“max
ps,a,h qPSˆAˆ/llbracketH/rrbracketCt
hps,aq,
where we exploited the fact that the expert’s policy is known in the last but one passage and used Pinsker’s inequality
in the last passage. When the US-IRL stops we have that max ps,a,h qPSˆAˆ/llbracketH/rrbracketCt
hps,aq ďǫand, consequently, for all
ps,a,h q PSˆAˆ/llbracketH/rrbracketwe have:
max
ps,a,h qPSˆAˆ/llbracketH/rrbracketCt
hps,aq “ max
ps,a,h qPSˆAˆ/llbracketH/rrbracket2?
2pH´h`1qd
β`
nt
hps,aq,δ˘
nt
hps,aqďǫ.
Thus, the algorithm stops at the smallest tsuch that:
ù ñnt
hps,aq ě8pH´h`1q2β`
nt
hps,aq,δ˘
ǫ2“8pH´h`1q2
ǫ2`
logpSAH {δq ` pS´1qlogpep1`nt
hps,aq{pS´1qq˘
.
Thus, by applying Lemma 15 of ( Kaufmann et al. ,2021 ), we obtain:
nτ
hps,aq ď8pH´h`1q2
ǫ2ˆ
logˆSAH
δ˙
` pS´1qlogˆ8epH´h`1q2
pS´1qǫ2ˆ
logˆSAH
δ˙
`4e˙˙˙
.
By recalling that τ“SAHnτ
hps,aq, and bounding H´h`1ďH, we obtain:
τď8H3SA
ǫ2ˆ
logˆSAH
δ˙
` pS´1qlogˆe
S´1`8eH2
pS´1qǫ2ˆ
logˆSAH
δ˙
`4e˙˙˙
.Towards Theoretical Understanding of Inverse Reinforceme nt Learning
If the transition model is time-homogeneous, we suppress th e subscript hand the algorithm US-IRL , will merge together
all the samples collected at different stages h. Let us deﬁne ntps,aq “řH
h“1nt
hps,aqandntps,a,s1q “řH
h“1nt
hps,a,s1q.
Now the transition model will be estimated straightforward ly as follows:
pptps1|s,aq:“#ntps,a,s1q
ntps,aqifntps,aq ą0
1
Sotherwise.
Let us consider now the following good event:
rE:“#
@tPN,@ps,aq PSˆA:DKL´
pptp¨|s,aq,pp¨|s,aq¯
ďrβ`
ntps,aq,δ˘
ntps,aq+
.
Thanks to Lemma B.4, we have that PpM,πEq,AprEq ě1´δ. Thus, in such a case, thanks to Theorem 3.2, we have:
HdGpR,pRτq ď2?
2 max
ps,a,h qPSˆAˆ/llbracketH/rrbracketpH´h`1qc
DKL´
ppt
hp¨|s,aq,php¨|s,aq¯
“max
ps,a,h qPSˆAˆ/llbracketH/rrbracketrCt
hps,aq.
The algorithm, therefore, stops as soon as:
max
ps,a,h qPSˆAˆ/llbracketH/rrbracketrCt
hps,aq “ max
ps,a,h qPSˆAˆ/llbracketH/rrbracket2?
2pH´h`1qd
rβ`
ntps,aq,δ˘
ntps,aq“max
ps,aqPSˆA2?
2Hd
rβ`
ntps,aq,δ˘
ntps,aqďǫ.
This allows us to compute the maximum value of nτps,aq:
nτps,aq ď8H2
ǫ2ˆ
logˆSA
δ˙
` pS´1qlogˆe
S´1`8eH2
pS´1qǫ2ˆ
logˆSA
δ˙
`4e˙˙˙
.
Recalling that τ“SAnτps,aq, we obtain:
τď8H2SA
ǫ2ˆ
logˆSA
δ˙
` pS´1qlogˆ8eH2
pS´1qǫ2ˆ
logˆSA
δ˙
`4e˙˙˙
.
Lemma B.4. The following statements hold:
•forβ`
n,δ˘
“logpSAH {δq ` pS´1qlog`
ep1`n{pS´1q˘
, we have that PpEq ě1´δ;
•forrβ`
n,δ˘
“logpSA{δq ` pS´1qlog`
ep1`n{pS´1q˘
, we have that PprEq ě1´δ.
Proof. Let us start with the ﬁrst statement. Similarly to Lemma 10 of (Kaufmann et al. ,2021 ), we apply ﬁrst a union bound
and then technical Proposition 1 of ( Jonsson et al. ,2020 ) (also reported as Lemma E.3for completeness) to concentrate
the KL-divergence:
PpEcq “P˜
DtPN,Dps,a,h q PSˆAˆ/llbracketH/rrbracket:DKL´
ppt
hp¨|s,aq,php¨|s,aq¯
ěβ`
nt
hps,aq,δ˘
nt
hps,aq¸
ďÿ
hP/llbracketH/rrbracketÿ
ps,aqPSˆAP˜
DtPN:DKL´
ppt
hp¨|s,aq,php¨|s,aq¯
ěβ`
nt
hps,aq,δ˘
nt
hps,aq¸
ďÿ
hP/llbracketH/rrbracketÿ
ps,aqPSˆAδ
SAH“δ.
The proof of the second statement is analogous having simply observed that the union bound has to be performed over
SˆAonly.Towards Theoretical Understanding of Inverse Reinforceme nt Learning
C. Examples of Section 3.2
In this appendix, we provide a detailed derivations of the ex amples presented in Section 3.2.
Example 3.1 (State-only reward rhpsq).State-only reward functions have been widely considered in many IRL ap-
proaches (e.g., Ng & Russell ,2000 ;Abbeel & Ng ,2004 ;Syed & Schapire ,2007 ;Komanduru & Honorio ,2019 ). We for-
malize the state-only feasible reward set as follows:
Rstate“RX t@ps,a,a1,hq:rhps,aq “rhps,a1qu.
Consider the MDP \R of Figure 1awithH“2,πE
hps0q“pπE
hps0q“a1withhPt1,2u. Setp1ps`|s0,a1q“1{2`ǫ{4and
pp1ps`|s0,a1q“1{2´ǫ{4and, thus, }p1p¨|s0,a1q´pp1p¨|s0,a1q}1“ǫ. Let us set r2ps`q“1andr2ps´q“´1, which makes
πEoptimal under p. We observe that pRis deﬁned by pr2ps´qďpr2ps`q. Recalling that the rewards are bounded in r´1,1s,
we haveHdGpRstate,pRstateqě1.
Proof. For the MDP \RM, in order to make πE
1ps0q “a1optimal, we have to enforce:
r1ps0q `2`ǫ
4r2ps`q `2´ǫ
4r2ps´q ěr1ps0q `1
2r2ps`q `1
2r2ps´q
ù ñr2ps`q ěr2ps´q.
Similarly, to make pπE
1ps0q “a1, we have for xM:
pr1ps0q `2´ǫ
4pr2ps`q `2`ǫ
4pr2ps´q ěpr1ps0q `1
2pr2ps`q `1
2pr2ps´q
ù ñpr2ps`q ďpr2ps´q.
Thus, suppose, we set r2ps´q “1andr2ps`q “ ´1, we have:
HdGpRstate,pRstateq ě min
pr2ps´q,pr2ps`qPr´1,1s
pr2ps`qďpr2ps´qmax t|1´pr2ps´q|,| ´1´pr2ps`q|u “1.
Example 3.2 (Time-homogeneous reward rps,aq).Time-homogeneous reward functions have been employed in se veral
RL (e.g., Dann & Brunskill ,2015 ) and IRL settings (e.g., Lindner et al. ,2022 ). We formalize the time-homogeneous feasible
reward set as follows:
Rhom“RX t@ps,a,h,h1q:rhps,aq “rh1ps,aqu.
Consider the MDP \R of Figure 1bwithH“2,πE
1ps0q“pπE
1ps0q“a1andπE
2ps0q“pπE
2ps0q“a2. ForhPt1,2u, we
setphps0|s0,a1q“1{2`ǫ{4andpphps0|s0,a1q“1{2´ǫ{4, thus, }php¨|s0,a1q´pphp¨|s0,a1q}1“ǫ. We set rps0,a1q“1,
rps0,a2q“1´ǫ{6, andrps1,a1q“rps1,a2q“1{2makingπEoptimal. We can prove that HdGpRhom,pRhomqě1{4.
Proof. Consider the MDP \RMand we set rps0,a1q “1,rps0,a2q “1´ǫ{12, andrps1,aq “1{2foraP ta1,a2u. We
immediately observe that πEis optimal since for h“2,rps0,a1q ěrps0,a2qand forh“1:
rps0,a2q `2`ǫ
4rps0,a1q `2´ǫ
4rps1,aq ěrps0,a1q `1
2rps0,a1q `1
2rps1,aq
ð ñrps0,a2q `´ǫ
4´1¯
rps0,a1q ´ǫ
4rps1,aq ě0
ð ñ1´ǫ
12`ǫ
4´1´ǫ
8ě0.
Consider now the alternative MDP \RxM, we have to enforce the following two conditions:
prps0,a1q ěprps0,a2q, (13)
prps0,a2q `2´ǫ
4prps0,a1q `2`ǫ
4prps1,aq ěprps0,a1q `1
2prps0,a1q `1
2prps1,aqTowards Theoretical Understanding of Inverse Reinforceme nt Learning
ð ñprps0,a2q ´´ǫ
4`1¯
prps0,a1q `ǫ
4prps1,aq ě0. (14)
The way of enforcing Equation ( 13) that is less constraining for Equation ( 14) is setting prps0,a1q “prps0,a2q, to get:
´ǫ
4prps0,a1q `ǫ
4prps1,aq ě0ð ñprps1,aq ěprps0,a1q.
This implies:
HdGpRhom,pRhomq ě min
prps1,aq,prps0,a1qPr´1,1s
prps1,aqěprps0,a1qmax"
|1´prps0,a1q|,ˇˇˇˇ1
2´prps1,aqˇˇˇˇ*
ě1
4,
by setting prps0,a1q “prps1,aq “1{4.
Example 3.3 (β-margin reward) .Aβ-margin reward enforces a suboptimality gap of at least βą0(Ng & Russell ,2000 ;
Komanduru & Honorio ,2019 ). We formalize it in the ﬁnite-horizon case with a sequence β“ pβhqhP/llbracketH/rrbracket, possibly different
for every stage:
Rβ-mar“RXt@ps,a,h q:AπE
hps,a;rqPt0uYp´8,´βhsu.
Consider the MDP \R in Figure 1awithπE
hps0q “pπE
hps0q “a1forhP t1,2u. We setp1ps`|s0,a1q “1{2`ǫand
pp1ps`|s0,a1q “1{2´ǫ. We set for MDP \RMthe reward function as r1ps0,aq “0andrhps`,aq “ ´rhps´,aq “1
foraP ta1,a2uandhP/llbracket2,H/rrbracket. Inps0,1qthe suboptimality gap is β1“2`2ǫpH´1q. By selecting Hě1`1{ǫ, the
feasible set pRβ-maris empty.
Proof. Concerning the MDP \RM, we observe that by setting r1ps0,a1q “1,r1ps0,a2q “ ´1, andrhps`,aq “
´rhps´,aq “1foraP ta1,a2uandhP/llbracket2,H/rrbracket, the policy πEis optimal. In particular, in state-stage pair ps0,1q
the suboptimality gap is given by β1“2`2ǫpH´1q. To enforce the optimality of pπE“πEin the MDP \RxM, we have:
pr1ps0,a1q `Hÿ
h“21
2prhps`,a1q `1
2prhps´,a1q ěpr1ps0,a2q `Hÿ
h“21
2prhps`,a1q `1
2prhps´,a1q `β1
ð ñpr1ps0,a1q ´pr1ps0,a2q ěβ1.
Thus, ifβ1ě2, we have that the feasible set pRβ-sepis empty. Thus, we select Hě1`1{ǫto haveβ1ě4.
D. Unknown Expert’s Policy πE
In this appendix, we extend the lower bounds and the algorith m for the case in which the expert’s policy is unknown.
Clearly, if the expert’s policy is deterministic, under the generative model setting, its estimation is trivial as it su fﬁces to
query every state and stage (resp. state) exactly once for ti me-inhomogeneous (resp. time-homogeneous) policies, lea ding
toEpM,πEq,Arτs “HS(resp.EpM,πEq,Arτs “S). Thus, we consider a more general setting in which the exper t’s policy
can be stochastic (still being optimal). Speciﬁcally, we co nsider the following assumption.
Assumption D.1. There exists a known constant πminP p0,1ssuch that every action played by the expert’s policy πEis
played with at least probability πmin:
@ps,a,h q PSˆAˆ/llbracketH/rrbracket:πE
hpa|sq P t0u Y rπmin,1s.
Intuitively, Assumption D.1formalizes a form of identiﬁability for the policy. As alrea dy mentioned in Section 3, what
matters for learning the feasible reward set is whether an ac tion is played by the agent (not the corresponding probabili ty).
Assumption D.1enforces that every optimal action must be played with a mini mum (known) non-null probability πmin.
We shall show that if this assumption is violated, the proble m becomes non-learnable.
D.1. Lower Bound
The following result provides a lower bound for learning the feasible reward set according to the PAC requirement of
Deﬁnition ( 4.1) when the expert’s policy is unknown, but the transition mod el is known. Clearly, one can combine thisTowards Theoretical Understanding of Inverse Reinforceme nt Learning
result with the ones of Section 5to address the setting in which both the expert’s policy and t he transition model are
unknown.
Theorem D.1. LetA“ pµ,τqbe an pǫ,δq-PAC algorithm for dG-IRL. Then, there exists an IRL problem pM,πEqwhere
πEfulﬁlls Assumption D.1such that, if ǫď1{2,δă1{16,Sě7,Aě2, andHě3, the number of samples Nis lower
bounded in expectation by:
•if the expert’s policy πEis time-inhomogeneous:
E
pM,πEq,Arτs ěSH
8log1
1´πminlogˆ1
δ˙
.
•if the expert’s policy πEis time-homogeneous:
E
pM,πEq,Arτs ěS
4log1
1´πminlogˆ1
δ˙
;
Before presenting the proof, let us comment the result. We ob serve that when Assumption D.1is violated, i.e., πminÑ0,
the sample complexity lower bound degenerates to inﬁnity, p roving that the problem become non-learnable.
Proof. Step 1: Instances Construction The hard MDP \R instances are depicted in Figure 6in a semi-formal way. The
state space is given by S“ tsstart,sroot,s1,...,sS,ssinkuand the action space is given by A“ ta0,a1,...,aAu. The
transition model is described below and the horizon is Hě3. We introduce the constant HP/llbracketH/rrbracket, whose value will be
chosen later. Let us observe, for now, that if H“1, the transition model is time-homogeneous.
The agent begins in state sstart, where every action has the same effect. Speciﬁcally, if the stagehăH, then there is
probability 1{2to remain in sstartand a probability 1{2to transition to sroot. Instead, if hěH, the state transitions to
srootdeterministically. From state sroot, every action has the same effect and the state transitions w ith equal probability
1{Sto a state siwithiP/llbracketS/rrbracket. In all states si, apart from a speciﬁc one, i.e., state s˚, the expert’s policy plays action a0
deterministically, i.e., πE
hpa0|siq “1and the state transitions deterministically to ssink. In states˚the expert’s policy plays
a0as the other ones if the stage h‰h˚, whereh˚P/llbracketH/rrbracketis a predeﬁned stage. If, instead, h“h˚, the expert’s action
playsa0w.p.1´πminand a speciﬁc action a˚w.p.πminP r0,1{2s. Then, the transition is deterministic to state ssink.
Notice that, having ﬁxed H, the possible values of h˚aret3,...,2`Hu. Statessinkis an absorbing state.
Let us consider the base instance π0in which the expert’s policy always plays action a0deterministically.14Additionally,
by varying the pair ℓ:“ ps˚,h˚q P ts1,...,sSu ˆ/llbracket3,H`2/rrbracket“:J, we can construct the class of instances denoted by
M“ tπℓ:ℓP t0u YJu.
Step 2: Feasible Set Computation Let us consider an instance πℓPM, we now seek to provide a lower bound to the
Hausdorff distance HdGpRπ0,Rπℓq. To this end, we focus on the pair ℓ“ ps˚,h˚qand we enforce the convenience of
both actions a0anda˚over the other actions. Since both actions are played with no n-zero probability by the expert’s
policy, their value function must be the same. Let us denote w ithrℓPRπℓ, we must have for all ajR ta0,a˚u:
rℓ
h˚ps˚,a0q `Hÿ
l“h˚`1rℓ
lpssinkq ěrℓ
h˚ps˚,ajq `Hÿ
l“h˚`1rℓ
lpssinkq
ù ñrℓ
h˚ps˚,a0q ěrℓ
h˚ps˚,ajq,
rℓ
h˚ps˚,a0q `Hÿ
l“h˚`1rℓ
lpssinkq “rℓ
h˚ps˚,a˚q `Hÿ
l“h˚`1rℓ
lpssinkq
ù ñrℓ
h˚ps˚,a0q “rℓ
h˚ps˚,a˚q.
Consider now the base instance π0and denote with r0PRπ0. Here we have to enforce the convenience of action a0over
14In this construction, the MDP \R does not change across the instances, but what changes is th e expert’s policy. Thus, we parametrize
the instances through the policy rather than the MDP \R.Towards Theoretical Understanding of Inverse Reinforceme nt Learning
sstart
sroot
... ...s˚s1 sS
ssinkhăHw.p.1
2
w.p.1
SorhěH
w.p.1
Sw.p.1
Sregardless the action w.p.1
S
playa0w.p.1 playa0w.p.1h“h˚playa˚w.p.πmin h“h˚play w.p. 1´πmin
Figure 6. Semi-formal representation of the the hard instances MDP \R used in the proof of Theorem D.1.
all the others, including a˚:
r0
h˚ps˚,a0q `Hÿ
l“h˚`1rℓ
lpssinkq ěr0
h˚ps˚,ajq `Hÿ
l“h˚`1rℓ
lpssinkq
ù ñr0
h˚ps˚,a0q ěr0
h˚ps˚,ajq,
r0
h˚ps˚,a0q `Hÿ
l“h˚`1r0
lpssinkq ěr0
h˚ps˚,a˚q `Hÿ
l“h˚`1r0
lpssinkq
ù ñr0
h˚ps˚,a0q ěr0
h˚ps˚,a˚q.
In order to lower bound the Hausdorff distance, we perform a v alid assignment of the rewards for the base instance:
r0
h˚ps˚,a0q “1, r0
h˚ps˚,a˚q “ ´1, r0
h˚ps˚,ajq “ ´1.
Thus, the Hausdorff distance can be bounded as follows, havi ng renamed, for convenience x“rℓ
h˚ps˚,a0qandy“
rℓ
h˚ps˚,a˚q:
HdGpRπ0,Rπℓq ěmin
x,yPr´1,1s
x“ymaxt|x´1|,|y`1|u “1.Towards Theoretical Understanding of Inverse Reinforceme nt Learning
Step 3: Lower bounding Probability Let us consider an pǫ,δq-correct algorithm Athat outputs the estimated feasible
setpR. Thus, for every ıPJ, we can lower bound the error probability:
δě sup
allMMDP\R and expert policies πP
pM,πq,Aˆ
HdG´
Rπ,pR¯
ě1
2˙
ěsup
πPMP
pM,πq,Aˆ
HdG´
Rπ,pR¯
ě1
2˙
ěmax
ℓPt0,ıuP
pM,πℓq,Aˆ
HdG´
Rπℓ,pR¯
ě1
2˙
.
For every ıPJ, let us deﬁne the identiﬁcation function :
Ψı:“argmin
ℓPt0,ıuHdG´
Rπℓ,pR¯
.
LetP t0,ıu. IfΨı“, then,HdGpRπΨı,Rπq “0. Otherwise, if Ψı‰, we have:
HdG`
RπΨı,Rπ˘
ďHdG´
RπΨı,pR¯
`HdG´
pR,Rπ¯
ď2HdG´
pR,Rπ¯
,
where the ﬁrst inequality follows from triangular inequali ty and the second one from the deﬁnition of identiﬁcation fun ction
Ψı. From Equation ( 11), we have that HdG`
RπΨı,Rπ˘
ě1. Thus, it follows that HdG´
pR,Rπ¯
ě1
2. This implies the
following inclusion of events for P t0,ıu:
"
HdG´
pR,Rπ¯
ě1
2*
Ě tΨı‰u.
Thus, we can proceed by lower bounding the probability:
max
ℓPt0,ıuP
pMℓ,πq,Aˆ
HdG´
Rπℓ,pR¯
ě1
2˙
ěmax
ℓPt0,ıuP
pMℓ,πq,ApΨı‰ℓq
ě1
2„
P
pM0,πq,ApΨı‰0q `P
pMı,πq,ApΨı‰ıq
“1
2„
P
pM0,πq,ApΨı‰0q `P
pMı,πq,ApΨı“0q
,
where the second inequality follows from the observation th atmaxta,bu ě1
2pa`bqand the equality from observing
thatΨıP t0,ıu. We can now apply the Bretagnolle-Huber inequality ( Lattimore & Szepesv´ ari ,2020 , Theorem 14.2) (also
reported in Theorem E.1for completeness) with P“PpM0,πq,A,Q“PpM0,πq,A, andA“ tΨı‰0u:
P
pM0,πq,ApΨı‰0q `P
pMı,πq,ApΨı“0q ě1
2expˆ
´DKLˆ
P
pM0,πq,A,P
pMı,πq,A˙˙
.
Step 4: KL-divergence Computation LetMPM, we denote with PA,M,πthe joint probability distribution of all events
realized by the execution of the algorithm in the MDP \R (the presence of pis irrelevant as it does not change across the
different instances):
P
pM,πq,A“τź
t“1ρtpst,at,ht|Ht´1qphtps1
t|st,atqπE
htpaE
t|stq.
whereHt´1“ ps1,a1,h1,s1
1,aE
1,...,s t´1,at´1,ht´1,s1
t´1,aE
t´1qis the history. Let ıPI. Let us now move to the
KL-divergence between the instances π0andπıfor some ı“ ps˚,h˚q PJ:
DKL`
PpM0,πq,A,PpMı,πq,A˘
“E
pM0,πq,A«τÿ
t“1DKL`
π0
htp¨|stq,πı
htp¨|stq˘ﬀTowards Theoretical Understanding of Inverse Reinforceme nt Learning
ďE
pM0,πq,A”
Nτ
h˚ps˚qı
DKL´
π0
h˚p¨|s˚q,πı
h˚p¨|s˚q¯
ďlog1
1´πminE
pM0,πq,A”
Nτ
h˚ps˚,a˚qı
.
having observed that the transition models differ in ı“ ps˚,h˚qand deﬁned Nτ
h˚ps˚q “řτ
t“11tpst,htq “ ps˚,h˚quand
the last passage is obtained by explicitly computing the KL- divergence:
DKL´
π0
h˚p¨|s˚q,πı
h˚p¨|s˚q¯
“ÿ
aPAπ0
h˚pa|s˚qlog˜
π0
h˚pa|s˚q
πı
h˚pa|s˚q¸
“π0
h˚pa0|s˚qlog˜
π0
h˚pa0|s˚q
πı
h˚pa0|s˚q¸
“log1
1´πmin.
Putting all together, we have:
δě1
4expˆ
´log1
1´πminE
pM0,πq,A”
Nτ
h˚ps˚qı˙
ù ñE
pM0,πq,A”
Nτ
h˚ps˚qı
ělog1
4δ
log1
1´πmin.
Thus, summing over ps˚,a˚q PJ, we have:
E
pM0,πq,Arτs ěÿ
ps˚,a˚qPJE
pM0,πq,A”
Nτ
h˚ps˚,a˚qı
“ÿ
ps˚,a˚,h˚qPIpH´H´2q2log1
4δ
2ǫ2
“SHlog1
4δ
log1
1´πmin.
The number of states is given by S“ |S| “S`3. Let us ﬁrst consider the time-homogeneous case, i.e., H“1:
E
pM0,πq,Arτs ě pS´3qlog1
4δ
log1
1´πmin.
Forδă1{16,Sě7,Aě2,Hě2, we obtain:
E
pM0,πq,Arτs ěS
4log1
1´πminlog1
δ.
For the time-inhomogeneous case, instead, we select H“H{2, to get:
E
pM0,πq,Arτs ěpS´3qpH{2q
ǫ2log1
4δ
log1
1´πmin.
Forδă1{16,Sě7,Aě2,Hě2, we obtain:
E
pM0,πq,Arτs ěSH
8log1
1´πminlog1
δ.
D.2. Algorithm
In this appendix, we extend US-IRL to the expert’s policy estimation under Assumption D.1. The pseudocode is reported
in Algorithm 2. The interaction protocol follows the same principles of Al gorithm 1, with the only difference that the
conﬁdence function, now, must account for the policy estima tion, leading to the following function for every ps,a,h q PTowards Theoretical Understanding of Inverse Reinforceme nt Learning
Input: signiﬁcance δP p0,1q,ǫtarget accuracy
tÐ0,ǫ0Ð `8
whileǫtąǫdo
tÐt`SAH
Collect one sample from each ps,a,h q PSˆAˆ/llbracketH/rrbracket
UpdatepptandpπE,taccording to ( 3)
Updateǫt“max ps,a,h qPSˆAˆ/llbracketH/rrbracketCt
hps,aq(resp.rCt
hps,aq)
end while
Algorithm 2. UniformSampling-IRL ( US-IRL ) for time-inhomogeneous (resp. time-homogeneous ) transition models and expert’s poli-
cies.
SˆAˆ/llbracketH/rrbracket:15
Ct
hps,aq:“2pH´h`1q¨
˝1tnt
hpsqěmaxt1,ξpnt
hpsq,δ{2quu`d
2β`
nt
hps,aq,δ{2˘
nt
hps,aq˛
‚. (16)
where:
ξpn,δq:“logp2SAHn2{δq
logp1{p1´πminqq.
It is worth noting that we have distributed the conﬁdence δequally between the problem estimating the policy and that o f
estimating the transition model. The following theorem pro vides the sample complexity of US-IRL .
Theorem D.2 (Sample Complexity of US-IRL ).Letǫą0andδP p0,1q, under Assumption D.1,US-IRL ispǫ,δq-PAC
fordG-IRL and with probability at least 1´δit stops after τsamples with:
•if the transition model pand the expert’s policy πEare time-inhomogeneous:
τď8H3SA
ǫ2ˆ
logˆSAH
δ˙
` pS´1qC1˙
`SH`SH
logp1{p1´πminqqˆ
logˆ4SAH
δ˙
`C2˙
,
whereC1“logpe{pS´1q ` p8eH2q{ppS´1qǫ2qplogp2SAH {δq `4eqqandC2“2log´
logp4SAH {δq`2
logp1{p1´πminqq¯
.
•if the transition model pand the expert’s policy πEare time-homogeneous:
τď8H2SA
ǫ2ˆ
logˆSA
δ˙
` pS´1qC1˙
`SH`S
logp1{p1´πminqqˆ
logˆ4SA
δ˙
`C2˙
,
whererC1“logpe{pS´1q ` p8eH2q{ppS´1qǫ2qplogp2SA{δq `4eqqandrC2“2log´
logp4SA{δq`2
logp1{p1´πminqq¯
.
Before moving to the proof, let us observe that the result mat ches the rate of the lower bound of Theorem D.1up to
logarithmic terms.
Proof. We make use of the notation of the proof of Theorem D.2. We start with the case in which the transition model is
15As for the transition model, one can adapt the conﬁdence func tion for the case of stationary policy in straightforward wa y:
rCt
hps,aq:“2pH´h`1q¨
˝1tnt
hpsqěmaxt1,rξpntpsq,δ{2quu`d
2rβ`
ntps,aq,δ{2˘
ntps,aq˛
‚, (15)
where:
rξpn,δq:“logp2SAn2{δq
logp1{p1´πminqq.
In principle, one can also consider the case of a time-homoge neous transition model and time-inhomogeneous expert’s po licy. We omit
it because it adds nothing to the characteristics of the prob lem and of the algorithms.Towards Theoretical Understanding of Inverse Reinforceme nt Learning
time-inhomogeneous. In addition to the good event Erelated to the transition model, we introduce the following one:
Eπ:“!
@tPN,@ps,a,h q PSˆAˆ/llbracketH/rrbracket:ˇˇˇ1πE
hpa|sq“0´1pπE,t
hpa|sq“0ˇˇˇď1tnt
hpsqěmaxt1,ξpnt
hpsq,δ{2quu)
,
whereπE
his the true expert’s policy and pπE,tis its estimate via Equation ( 3) at time t. Thanks to Lemma B.4and
Lemma D.3, we have that PpEXEπq ě1´δ. Thus, under the good event EXEπ, we apply Theorem 3.2to obtain
HdGpR,pRτq ďmax ps,a,h qPSˆAˆ/llbracketH/rrbracketCt
hps,aq. A sufﬁcient condition to make this term ďǫis to request the following
ones:
max
ps,a,h qPSˆAˆ/llbracketH/rrbracket2pH´h`1q1tnt
hpsqěmaxt1,ξpnt
hpsq,δ{2quu“0,
max
ps,a,h qPSˆAˆ/llbracketH/rrbracket2?
2pH´h`1qd
β`
nt
hps,aq,δ{2˘
nt
hps,aqďǫ.
For the ﬁrst one, we ﬁrst enforce the condition:
nt
hpsq ěξpnt
hpsq,δ{2q “logp4SAH pnt
hq2{δq
logp1{p1´πminqq“logp4SAH {δq
logp1{p1´πminqq`2lognt
h
logp1{p1´πminqq.
Using Lemma 15 of ( Kaufmann et al. ,2021 ) and enforcing nt
hpsq ě1, we obtain:
nτ
hpsq ď1`1
logp1{p1´πminqqˆ
logp4SAH {δq `2logˆlogp4SAH {δq `2
logp1{p1´πminqq˙˙
.
Combining this result with that of Theorem D.2for what concerns the transition model, we obtain:
τď8H3SA
ǫ2ˆ
logˆ2SAH
δ˙
` pS´1qlogˆ8eH2
pS´1qǫ2ˆ
logˆ2SAH
δ˙
`4e˙˙˙
`SH`SH
logp1{p1´πminqqˆ
logp4SAH {δq `2logˆlogp4SAH {δq `2
logp1{p1´πminqq˙˙
.
Analogous derivations can be carried out for the case of time -homogenous policy using the good event:
rEπ:“!
@tPN,@ps,aq PSˆA:ˇˇ1πEpa|sq“0´1pπE,tpa|sq“0ˇˇď1tntpsqěmaxt1,rξpntpsq,δ{2quu)
,
whererξpn,δq:“logp2SAn2{δq
logp1{p1´πminqq. We omit the tedious but straightforward derivation.
Lemma D.3. Under Assumption D.1, the following statements hold:
•forξpn,δq:“logp2SAHn2{δq
logp1{p1´πminqq, we have that PpEπq ě1´δ;
•forrξpn,δq:“logp2SAn2{δq
logp1{p1´πminqq, we have that PprEπq ě1´δ.
Proof. Let us start with the ﬁrst statement. We apply ﬁrst a union bou nd and, then, Lemma E.5to perform the concentra-
tion:
PpEc
πq “P˜
DtPN,Dps,a,h q PSˆAˆ/llbracketH/rrbracket:ˇˇˇ1πE
hpa|sq“0´1pπE,t
hpa|sq“0ˇˇˇď1tnt
hpsqąmaxt1,ξpnt
hpsq,δquu¸
“P˜
DnPN,Dps,a,h q PSˆAˆ/llbracketH/rrbracket:ˇˇˇ1πE
hpa|sq“0´1pπE,rns
hpa|sq“0ˇˇˇą1tněmax t1,ξpn,δquu¸
ďÿ
hP/llbracketH/rrbracketÿ
ps,aqPSˆAÿ
ně0P˜ˇˇˇ1πE
hpa|sq“0´1pπE,rns
hpa|sq“0ˇˇˇď1tnąmax t1,ξpn,δquu¸Towards Theoretical Understanding of Inverse Reinforceme nt Learning
ďÿ
hP/llbracketH/rrbracketÿ
ps,aqPSˆAÿ
ně1P˜ˇˇˇ1πE
hpa|sq“0´1pπE,rns
hpa|sq“0ˇˇˇď1tnąmax t1,ξpn,δquu¸
ďÿ
hP/llbracketH/rrbracketÿ
ps,aqPSˆAδ
2SAHn2“π2
6δ
2ďδ.
where on the ﬁrst passage we enforced the condition on the tim e instants in which the policy estimate changes (i.e., when
ps,hqis visited) and we denotes such an estimate as pπE,rns
h. Then, after a union bound, we apply Lemma E.5. The proof of
the second statement is analogous having simply observed th at the union bound has to be performed over SˆAonly.
E. Technical Lemmas
Theorem E.1. (Bretagnolle-Huber inequality (Lattimore & Szepesv ´ari,2020 , Theorem 14.2)) Let PandQbe probability
measures on the same measurable space pΩ,Fq, and letAPFbe an arbitrary event. Then,
PpAq `QpAcq ě1
2expp´DKLpP,Qqq,
whereAc“ΩzAis the complement of A.
Theorem E.2. (Fano inequality (Gerchinovitz et al. ,2017 , Proposition 4)) Let P0,P1,...,PMbe probability measures on
the same measurable space pΩ,Fq, and letA1,...,AMPFbe a partition of Ω. Then,
1
MMÿ
i“1PipAc
iq ě1´1
MřM
i“1DKLpPi,P0q ´log2
logM,
whereAc“ΩzAis the complement of A.
Lemma E.3. (Jonsson et al. ,2020 , Proposition 1) Let P“ pp1,...,p Dqbe a categorical probability measure on the
support /llbracketD/rrbracket. LetPn“ ppp1,...,ppDqbe the maximum likelihood estimate of Pobtained with ně1independent samples.
Then, for every δP p0,1qit holds that:
PpDně1 :nD KLpPn,Pq ąlogp1{δq ` pD´1qlogpep1`n{pD´1qqqq ďδ
Lemma E.4. LetǫP r0,1{2sandvP t´ǫ,ǫuDsuch thatřd
i“1vi“0. Consider the two categorical distributions
P“`1
D,1
D,...,1
D˘
andP“`1`v1
D,1`v2
D,...,1`vD
D˘
. Then, it holds that:
DKLpP,Qq ď2ǫ2andDKLpQ,Pq ď2ǫ2.
Proof. First of all we recall that sinceřM
i“1vi“0, we have |tiP/llbracketD/rrbracket:vi“ǫu| “ |iP/llbracketD/rrbracket:vi“ ´ǫ| “D{2. Let us
compute the KL-divergence DKLpP,Qq:
DKLpP,Qq “Dÿ
i“11`vi
Dlog1`vi
D
1
D
“ÿ
iP/llbracketD/rrbracket:vi“ǫ1`ǫ
Dlogp1`ǫq `ÿ
iP/llbracketD/rrbracket:vi“´ǫ1´ǫ
Dlogp1´ǫq
“1`ǫ
2logp1`ǫq `1´ǫ
2logp1´ǫq
“1
2logp1´ǫ2qloooooomoooooon
ď0`ǫ
2logp1`ǫq ´ǫ
2logp1´ǫq
ďǫ2
2`ǫ
2ˆ1
1´ǫ´1˙
“ǫ22´ǫ
2p1´ǫqď3
2ǫ2ď2ǫ2.
where we used the inequality logp1`xq ďxforxě0and´logp1´xq ď1
1´x´1for0ăxă1and exploited thatTowards Theoretical Understanding of Inverse Reinforceme nt Learning
ǫď1
2. Let us now move to the second KL-divergence DKLpQ,Pq:
DKLpQ,Pq “Dÿ
i“11
Dlog1
D
1`vi
D
“ÿ
iP/llbracketD/rrbracket:vi“ǫ1
Dlog1
1`ǫ`ÿ
iP/llbracketD/rrbracket:vi“´ǫ1
Dlog1
1´ǫ
“ ´1
2logp1´ǫ2q
ď1
2ˆ1
1´ǫ2´1˙
“ǫ2
2p1´ǫ2qď2
3ǫ2ď2ǫ2,
where we used the inequality ´logp1´xq ď1
1´x´1for0ăxă1and observed that ǫď1
2.
Lemma E.5. LetP“ pp1,...,p Dqbe a categorical probability measure on the support /llbracketD/rrbracket. LetPn“ ppp1,...,ppDqbe
the maximum likelihood estimate of Pobtained with ně1independent samples. Then, if piP t0u Y rpmin,1sfor some
pminP p0,1s. Then, for every iP/llbracketD/rrbracketindividually, for every δP p0,1q, it holds that:
ˇˇ1tpi“0u´1tppi“0uˇˇď1#
němax#
1,logp1
δq
logp1
1´pminq++.
Proof. LetiP/llbracketD/rrbracketsuch that pią0and, thus,1tpi“0u“0. By assumption, it must be that piěpmin. To make a mistake,
we must have that 1tppi“0u“1, and, thus, ppi“0. Thus, we compute the probability that no sample iis observed among
thenones:
P¨
˝č
jP/llbracketn/rrbracketXj‰i˛
‚“ź
jP/llbracketn/rrbracketPpXj‰iq “PpX1‰iqn“ p1´piqnď p1´pminqn,
where we exploited the fact that the random variables Xjare i.i.d.. If n“0the latter expression is 1. If, instead, ně1,
by setting the last expression equal to δ, we get:
p1´pminqnďδù ñnělog`1
δ˘
log´
1
1´pmin¯.
The result follows.
Lemma E.6. LetV“ tvP t´1,1uD:řD
j“1vj“0u. Then, theD
16-packing number of Vw.r.t. the metric dpv,v1q “řD
j“1|vj´v1
j|is lower bounded by 2D
5.
Proof. Let us denote the packing number with Mpǫ;V,dqand the covering number with Npǫ;V,dq. It is well known that
Npǫ;V,dq ďMpǫ;V,dq(Gy¨ orﬁ et al. ,2002 ). Thus, a lower bound to the covering number is a lower bound t o the packing
number. Let us consider the (pseudo)metric d1pv,v1q “řD{2
j“1|vj´v1
j|that considers the ﬁrst half of the components
only. Clearly, we have that d1pv,v1q ďdpv,v1q. Therefore, any ǫ-cover w.r.t. dpv,v1qis anǫ-cover w.r.t. d1pv,v1qand,
consequently, Npǫ;V,d1q ďNpǫ;V,dq. Since the (pseudo)metric d1considers only the ﬁrst half of the components,
constructing an ǫ-cover of Vw.r.t.d1is equivalent to constructing an ǫ-cover of V1w.r.t.d1, whereV1“ t´1,1uD{2.
V1considers the ﬁrst half of the components of vectors of V, that can be freely chosen, disregarding the summation
constraint.16Thus,Npǫ;V,d1q “Npǫ;V1,d1q. Notice that d1is now a proper metric on V1“ t´1,1uD{2. Now, we reduce
the problem to constructing cover on the Hamming space H“ t0,1uD{2. Indeed, we can always map an pǫ{2q-cover for
the Hamming space Hto anǫ-cover for the space V1. Speciﬁcally, let phlqlanpǫ{2q-cover for the Hamming space, we
16From an algebraic perspective, V1can be considered the quotient set obtained from Vby means of the equivalence relation v„
v1ð ñvj“vj1for alljP/llbracketD{2/rrbracket.Towards Theoretical Understanding of Inverse Reinforceme nt Learning
construct pv1
lqlby applying the following transformation:
v1
j“#
´1ifhj“0
1 ifhj“1,
or, in more convenient way, v1“2h´1. Letv1PV1:
min
ld1pv1,v1
l,jq “min
lD{2ÿ
j“1|v1
j´v1
l,j| “2min
lD{2ÿ
j“1|h1
j´h1
l,j| ďǫ.
The covering number of a Hamming space has been lower bounded in (Cohen & Frankl ,1985 ) forǫP/llbracketD{2/rrbracketas:
log2Npǫ;H,d1q ěD
2´log2ǫÿ
k“0ˆD{2
k˙
.
We takeǫ“D{16, and we use the known boundřk
i“0`n
k˘
ď`en
k˘k:
D{16ÿ
k“0ˆD{2
k˙
ď p8eqD{16.
From, which, we get:
log2Npǫ;H,d1q ěD
2´log2ǫÿ
k“0ˆD{2
k˙
ěD
2´D
16log2p8eq ěD
5.